{% extends "base.html" %}
{% block title %}Project Overview - Jal Sarovar{% endblock %}

{% block extra_css %}
<style>
    @media print {
        .no-print { display: none !important; }
        .page-break { page-break-before: always; }
        body { font-size: 10pt; }
        .card { border: 1px solid #ddd !important; }
    }

    .research-paper {
        max-width: 900px;
        margin: 0 auto;
        background: white;
        padding: 40px;
    }

    .paper-title {
        text-align: center;
        margin-bottom: 30px;
    }

    .paper-title h1 {
        font-size: 1.8rem;
        color: #333;
        margin-bottom: 10px;
    }

    .paper-subtitle {
        font-size: 1rem;
        color: #666;
    }

    .section-title {
        font-size: 1.3rem;
        color: #2c5282;
        border-bottom: 2px solid #2c5282;
        padding-bottom: 5px;
        margin-top: 30px;
        margin-bottom: 15px;
    }

    .subsection-title {
        font-size: 1.1rem;
        color: #4a5568;
        margin-top: 20px;
        margin-bottom: 10px;
    }

    .data-table {
        width: 100%;
        border-collapse: collapse;
        margin: 15px 0;
        font-size: 0.9rem;
    }

    .data-table th, .data-table td {
        border: 1px solid #e2e8f0;
        padding: 8px 12px;
        text-align: left;
    }

    .data-table th {
        background-color: #f7fafc;
        font-weight: 600;
    }

    .data-table tr:nth-child(even) {
        background-color: #f7fafc;
    }

    /* Smooth scrolling for section navigation */
    html {
        scroll-behavior: smooth;
    }

    /* Add padding-top to sections to account for fixed header */
    .section-title {
        scroll-margin-top: 20px;
    }

    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border-radius: 8px;
        padding: 15px;
        text-align: center;
    }

    .metric-card h4 {
        font-size: 0.85rem;
        opacity: 0.9;
        margin-bottom: 5px;
    }

    .metric-card .value {
        font-size: 1.8rem;
        font-weight: bold;
    }

    .pseudocode {
        background: #1a202c;
        color: #e2e8f0;
        padding: 15px;
        border-radius: 8px;
        font-family: 'Monaco', 'Consolas', monospace;
        font-size: 0.85rem;
        overflow-x: auto;
        white-space: pre-wrap;
    }

    .pseudocode .keyword {
        color: #f687b3;
    }

    .pseudocode .comment {
        color: #68d391;
    }

    .pseudocode .function {
        color: #63b3ed;
    }

    .io-table th {
        background: #edf2f7;
    }

    .comparison-badge {
        display: inline-block;
        padding: 2px 8px;
        border-radius: 4px;
        font-size: 0.75rem;
        font-weight: 600;
    }

    .badge-success { background: #c6f6d5; color: #22543d; }
    .badge-warning { background: #fefcbf; color: #744210; }
    .badge-info { background: #bee3f8; color: #2a4365; }

    .flowchart {
        background: #f7fafc;
        border: 1px solid #e2e8f0;
        border-radius: 8px;
        padding: 20px;
        text-align: center;
    }

    .flowchart-step {
        display: inline-block;
        padding: 10px 15px;
        background: #4299e1;
        color: white;
        border-radius: 6px;
        margin: 5px;
    }

    .flowchart-arrow {
        color: #4a5568;
        font-size: 1.2rem;
    }

    .abstract-box {
        background: #ebf8ff;
        border-left: 4px solid #4299e1;
        padding: 15px 20px;
        margin: 20px 0;
    }

    .toc-box {
        background: #f7fafc;
        border: 1px solid #e2e8f0;
        border-radius: 8px;
        padding: 20px;
        margin: 30px 0;
    }

    .toc-title {
        font-size: 1.2rem;
        font-weight: 600;
        color: #2c5282;
        margin-bottom: 15px;
    }

    .toc-list {
        list-style: none;
        padding-left: 0;
    }

    .toc-list li {
        margin: 8px 0;
    }

    .toc-list a {
        color: #2c5282;
        text-decoration: none;
        transition: all 0.2s;
    }

    .toc-list a:hover {
        color: #4299e1;
        text-decoration: underline;
    }

    .toc-subsection {
        list-style: none;
        padding-left: 20px;
        margin-top: 5px;
    }

    .toc-subsection li {
        margin: 5px 0;
        font-size: 0.9rem;
    }

    .back-to-top {
        display: inline-block;
        margin-top: 10px;
        font-size: 0.85rem;
        color: #4299e1;
        text-decoration: none;
    }

    .back-to-top:hover {
        text-decoration: underline;
    }
</style>
{% endblock %}

{% block content %}
<div class="no-print mb-4">
    <div class="d-flex justify-content-between align-items-center">
        <h2><i class="bi bi-file-earmark-text"></i> Project Overview</h2>
        <div>
            <button onclick="window.print()" class="btn btn-outline-primary">
                <i class="bi bi-printer"></i> Print / Save PDF
            </button>
            <a href="/Users/test/lab4all_wflow_RELEASE_RONALD/lab4all_webapp/research_paper/lab4all_research_paper.pdf" class="btn btn-primary" onclick="alert('Use Print/Save PDF button above, or download the LaTeX-generated PDF from the research_paper folder'); return false;">
                <i class="bi bi-file-pdf"></i> Download LaTeX PDF
            </a>
        </div>
    </div>
</div>

<div class="research-paper">
    <!-- Title Page -->
    <div class="paper-title">
        <h1>Jal Sarovar: A Comprehensive Dual-Scale Machine Learning Framework Integrating Autonomous Robots and IoT for Public and Residential Water Quality Monitoring</h1>
        <p class="paper-subtitle">Samridhi Chordia</p>
    </div>

    <!-- Abstract -->
    <h2 class="section-title">Abstract</h2>
    <div class="abstract-box">
        <p>
            This research presents a comprehensive machine learning framework for water quality monitoring in India,
            addressing critical limitations of traditional laboratory testing through a dual-scale monitoring paradigm.
            At the <strong>public scale</strong>, the LAS (Low-cost Autonomous System) robotic platform surveys geographically
            distributed water bodies (stepwells, tanks, ponds, rivers) with multi-parameter sensors (pH, TDS, turbidity,
            dissolved oxygen, temperature) and computer vision capabilities for population-level surveillance.
            At the <strong>residential scale</strong>, low-cost IoT sensors (₹8,000-15,000 per household) provide
            continuous real-time monitoring with 15-minute sampling intervals for immediate contamination alerts.
        </p>
        <p>
            The system integrates five production-ready machine learning models trained on a comprehensive dataset
            of {{ stats.total_samples }} water samples from {{ stats.total_sites }} monitoring sites spanning
            {{ stats.date_range_years }} years. Models achieve <strong>84.6% accuracy for site risk classification</strong>,
            <strong>73.4% for contamination type detection</strong>, and <strong>R²=0.94 for time-series forecasting</strong>,
            while demonstrating <strong>{{ stats.avg_cost_reduction }}% cost reduction</strong> through intelligent
            resource allocation compared to traditional exhaustive testing.
        </p>
        <p>
            A unique hybrid data architecture enables coexistence of manual data collection (CSV uploads, API integration,
            laboratory submissions) with autonomous sensor streams (LAS measurements, IoT readings), weighted by
            reliability for decision-making. The framework supports multiple government initiatives including
            <strong>Mission Amrit Sarovar</strong> (68,000+ water bodies nationally), <strong>Jal Jeevan Mission</strong>
            (rural drinking water safety), and state/municipal water quality programs.
        </p>
        <p>
            Following successful proof-of-concept deployment and ML model validation, the system is currently scaling
            autonomous monitoring infrastructure while maintaining continuous bulk data import from existing monitoring
            networks. This incremental transition preserves human expertise and laboratory validation as essential
            components of water safety decision-making.
        </p>
    </div>

    <!-- Table of Contents -->
    <div class="toc-box" id="table-of-contents">
        <div class="toc-title"><i class="bi bi-list-ul"></i> Table of Contents</div>
        <ul class="toc-list">
            <li><a href="#section-1">1. Introduction</a>
                <ul class="toc-subsection">
                    <li><a href="#section-1-1">1.1 Motivation and Project Objectives</a></li>
                    <li><a href="#section-1-2">1.2 The Dual-Scale Paradigm</a></li>
                    <li><a href="#section-1-3">1.3 ML-Driven Capabilities</a></li>
                    <li><a href="#section-1-4">1.4 Project Phases and Roadmap</a></li>
                </ul>
            </li>
            <li><a href="#section-2">2. System Overview</a>
                <ul class="toc-subsection">
                    <li><a href="#section-2-0">2.0 Data Sources and Collection Methodology</a></li>
                    <li><a href="#section-2-1">2.1 LAS: Autonomous Robotic Architecture</a></li>
                    <li><a href="#section-2-2">2.2 IoT Sensor Architecture</a></li>
                    <li><a href="#section-2-3">2.3 Data Summary</a></li>
                    <li><a href="#section-2-4">2.4 Water Quality Parameters</a></li>
                </ul>
            </li>
            <li><a href="#section-3">3. Site Risk Classifier (Random Forest)</a>
                <ul class="toc-subsection">
                    <li><a href="#section-3-0">3.0 How It Functions</a></li>
                    <li><a href="#section-3-1">3.1 Input Parameters</a></li>
                    <li><a href="#section-3-2">3.2 Internal Operations</a></li>
                    <li><a href="#section-3-3">3.3 Output Parameters</a></li>
                    <li><a href="#section-3-4">3.4 Pseudocode</a></li>
                </ul>
            </li>
            <li><a href="#section-4">4. Contamination Classifier (XGBoost)</a>
                <ul class="toc-subsection">
                    <li><a href="#section-4-0">4.0 How It Functions</a></li>
                    <li><a href="#section-4-1">4.1 Input Parameters</a></li>
                    <li><a href="#section-4-2">4.2 Internal Operations (Probability Scoring)</a></li>
                    <li><a href="#section-4-3">4.3 Output Parameters</a></li>
                    <li><a href="#section-4-4">4.4 Pseudocode</a></li>
                </ul>
            </li>
            <li><a href="#section-5">5. Water Quality Forecaster (Gaussian Process)</a>
                <ul class="toc-subsection">
                    <li><a href="#section-5-0">5.0 How It Functions</a></li>
                    <li><a href="#section-5-1">5.1 Input Parameters</a></li>
                    <li><a href="#section-5-2">5.2 Internal Operations</a></li>
                    <li><a href="#section-5-3">5.3 Output Parameters</a></li>
                    <li><a href="#section-5-4">5.4 Pseudocode</a></li>
                </ul>
            </li>
            <li><a href="#section-6">6. Bayesian Cost Optimizer</a>
                <ul class="toc-subsection">
                    <li><a href="#section-6-0">6.0 How It Functions</a></li>
                    <li><a href="#section-6-1">6.1 Input Parameters</a></li>
                    <li><a href="#section-6-2">6.2 Internal Operations</a></li>
                    <li><a href="#section-6-3">6.3 Output Parameters</a></li>
                    <li><a href="#section-6-4">6.4 Pseudocode</a></li>
                </ul>
            </li>
            <li><a href="#section-7">7. Real-time WQI Calculator (Rule-Based)</a>
                <ul class="toc-subsection">
                    <li><a href="#section-7-1">7.1 Input Parameters</a></li>
                    <li><a href="#section-7-2">7.2 Internal Operations (Penalty Scoring)</a></li>
                    <li><a href="#section-7-3">7.3 Output Parameters</a></li>
                    <li><a href="#section-7-4">7.4 Pseudocode</a></li>
                </ul>
            </li>
            <li><a href="#section-8">8. Hybrid Anomaly Detection (Isolation Forest + CUSUM)</a>
                <ul class="toc-subsection">
                    <li><a href="#section-8-0">8.0 Why Use TWO Detectors Together?</a></li>
                    <li><a href="#section-8-1">8.1 Part A: Isolation Forest - Sudden Anomaly Detection</a></li>
                    <li><a href="#section-8-2">8.2 Part B: CUSUM - Gradual Drift Detection</a></li>
                    <li><a href="#section-8-3">8.3 Hybrid Detection Strategy</a></li>
                </ul>
            </li>
            <li><a href="#section-9">9. Complete ML Analysis Pipeline</a></li>
            <li><a href="#section-10">10. Intervention Management and Treatment Effectiveness</a>
                <ul class="toc-subsection">
                    <li><a href="#section-10-1">10.1 Intervention Framework</a></li>
                    <li><a href="#section-10-2">10.2 Intervention Types</a></li>
                    <li><a href="#section-10-3">10.3 Effectiveness Measurement</a></li>
                    <li><a href="#section-10-4">10.4 Projected Results</a></li>
                    <li><a href="#section-10-5">10.5 Integration with ML Pipeline</a></li>
                </ul>
            </li>
            <li><a href="#section-11">11. Results Summary</a>
                <ul class="toc-subsection">
                    <li><a href="#section-11-1">11.1 Model Performance Metrics</a></li>
                    <li><a href="#section-11-2">11.2 Geographic Coverage</a></li>
                </ul>
            </li>
            <li><a href="#section-12">12. Conclusion</a>
                <ul class="toc-subsection">
                    <li><a href="#section-12-1">12.1 Implementation Roadmap</a></li>
                    <li><a href="#section-12-2">12.2 Current Achievements</a></li>
                    <li><a href="#section-12-3">12.3 Future Work and Scaling Strategy</a></li>
                </ul>
            </li>
            <li><a href="#section-13">13. References</a></li>
        </ul>
    </div>

    <!-- Introduction -->
    <h2 class="section-title" id="section-1">1. Introduction</h2>

    <h3 class="subsection-title" id="section-1-1">1.1 Motivation and Project Objectives</h3>
    <p>
        Safe drinking water is fundamental to human health, yet approximately 2 billion people globally lack access to safely
        managed drinking water services. In India alone, water contamination contributes to 37.7 million cases of waterborne
        diseases annually. Traditional water quality monitoring approaches face critical limitations: high cost (estimated Rs. 1,000-2,000
        per laboratory sample), delayed detection (monthly/quarterly testing misses rapid contamination events), and limited
        coverage preventing comprehensive monitoring of all water sources.
    </p>
    <p>
        <strong>Jal Sarovar</strong> addresses these challenges by supporting the goals of various Government initiatives in India
        at both State and Central levels, including:
    </p>
    <ul>
        <li><strong>Mission Amrit Sarovar:</strong> Supporting the development and rejuvenation of 75 water bodies in each district (approximately 68,000 water bodies nationwide)</li>
        <li><strong>Jal Jeevan Mission:</strong> Enabling water quality monitoring for safe and adequate drinking water to rural households</li>
        <li><strong>Municipal Administration and Urban Development Departments:</strong> Providing data-driven insights for urban water body management at State and Central levels</li>
        <li><strong>Water Supply and Drainage Boards:</strong> Supporting continuous monitoring and quality assurance for public water supply infrastructure</li>
    </ul>
    <p>
        Beyond supporting these national and state-level initiatives, the project establishes a framework for <strong>nationwide
        collaboration</strong> for tracking water quality in both public water bodies and private sites, enabling
        comprehensive monitoring at multiple scales and supporting evidence-based policy decisions with a truly national and global collaborative approach based on sharing of water quality parameters across regions.
    </p>

    <h3 class="subsection-title" id="section-1-2">1.2 The Dual-Scale Paradigm</h3>
    <p>
        Water quality management inherently operates at two distinct scales, each serving complementary purposes:
    </p>
    <div class="row g-3 mb-3">
        <div class="col-md-6">
            <div class="card h-100">
                <div class="card-header" style="background: #2c5282; color: white;">
                    <strong>Public Scale (Macro)</strong>
                </div>
                <div class="card-body">
                    <ul class="mb-0">
                        <li>Geographically distributed public water bodies (rivers, stepwells, tanks, ponds)</li>
                        <li>Periodic testing via LAS robots (weekly to quarterly)</li>
                        <li>Population-level surveillance and policy decisions</li>
                        <li>Cost-optimized resource allocation</li>
                    </ul>
                </div>
            </div>
        </div>
        <div class="col-md-6">
            <div class="card h-100">
                <div class="card-header" style="background: #38a169; color: white;">
                    <strong>Private Scale (Micro)</strong>
                </div>
                <div class="card-body">
                    <ul class="mb-0">
                        <li>Commercial and Residential communities</li>
                        <li>Continuous real-time monitoring (15-60 minute intervals)</li>
                        <li>Water safety assurance and immediate alerts</li>
                        <li>Consumer-grade affordability</li>
                    </ul>
                </div>
            </div>
        </div>
    </div>
    <p>
        These scales are complementary: <strong>public site monitoring</strong> identifies systemic contamination patterns and informs
        policy interventions, while <strong>private site monitoring</strong> provides granular safety verification and empowers
        institutions and individuals. This proof-of-concept validates the first integrated system combining both scales with coordinated
        ML analytics, supporting multiple initiatives' goals of comprehensive water body rejuvenation and establishing a
        model for national and global water quality collaboration.
    </p>

    <h3 class="subsection-title" id="section-1-3">1.3 ML-Driven Capabilities</h3>
    <p>
        Jal Sarovar provides a comprehensive ML pipeline with the following capabilities:
    </p>
    <ul>
        <li><strong>Bulk Data Import & Processing:</strong> CSV uploads, API integration, and mobile app submissions from NABL/ISO 17025 accredited laboratory test results</li>
        <li><strong>5 Production-Ready ML Models:</strong> Site Risk Classifier (84.6% accuracy), Contamination Classifier (73.4% accuracy), Water Quality Forecaster (R²=0.94), Bayesian Cost Optimizer, and Hybrid Anomaly Detector</li>
        <li><strong>LAS Autonomous Robotic Testing:</strong> Multi-parameter water quality sensors with computer vision for algae detection deployed on public water bodies</li>
        <li><strong>IoT Real-Time Monitoring:</strong> Low-cost inline sensors (₹8,000-15,000) for continuous household water quality monitoring with 15-minute sampling intervals</li>
        <li><strong>Hybrid Data Architecture:</strong> Multi-source data fusion combining manual laboratory testing with autonomous sensor streams, weighted by reliability (laboratory > LAS > IoT)</li>
    </ul>

    <p class="mt-2">
        <em>See <a href="#section-12-1">Section 12.1 Implementation Roadmap</a> for detailed phased deployment timeline.</em>
    </p>

    <h3 class="subsection-title" id="section-1-4">1.4 Implementation Phases</h3>

    <p>
        The Jal Sarovar framework development follows a six-phase incremental approach, progressing from
        foundational infrastructure to long-term hybrid operations. A detailed <strong>Implementation Roadmap</strong>
        with status, milestones, and timelines is provided in <a href="#section-12-1">Section 12.1</a>.
    </p>

    <div class="row mt-3">
        <div class="col-md-4">
            <div class="card mb-3">
                <div class="card-header bg-success text-white">
                    <strong>Phases 1-2: Foundation</strong>
                    <span class="badge bg-light text-success float-end">COMPLETED</span>
                </div>
                <div class="card-body">
                    <small>
                        • Preliminary trials and data infrastructure<br>
                        • ML model development and validation<br>
                        • Production deployment readiness
                    </small>
                </div>
            </div>
        </div>

        <div class="col-md-4">
            <div class="card mb-3">
                <div class="card-header bg-primary text-white">
                    <strong>Phase 3: Ramp-Up</strong>
                    <span class="badge bg-light text-primary float-end">IN PROGRESS</span>
                </div>
                <div class="card-body">
                    <small>
                        • Expanding LAS and IoT deployment<br>
                        • Real-time ML model integration<br>
                        • Sensor accuracy validation
                    </small>
                </div>
            </div>
        </div>

        <div class="col-md-4">
            <div class="card mb-3">
                <div class="card-header" style="background-color: #ffc107;">
                    <strong>Phases 4-6: Scale & Operations</strong>
                    <span class="badge bg-light text-dark float-end">PLANNED/ONGOING</span>
                </div>
                <div class="card-body">
                    <small>
                        • Scale-up to 50+ sites and 500+ households<br>
                        • Long-term hybrid data collection model<br>
                        • Government program integration
                    </small>
                </div>
            </div>
        </div>
    </div>

    <p class="mt-2">
        <em>See <a href="#section-12-1">Section 12.1 Implementation Roadmap</a> for complete phase details,
        technical specifications, and timelines.</em>
    </p>

    <!-- System Overview -->
    <h2 class="section-title" id="section-2">2. System Overview</h2>

    <h3 class="subsection-title" id="section-2-0">2.0 Data Sources and Collection Methodology</h3>

    <h4>Data Collection Sources</h4>

    <p>
        The Jal Sarovar system operates on a <strong>hybrid data collection model</strong> combining multiple sources.
        <strong>Bulk imported laboratory test results</strong> from {{ stats.total_sites }} public monitoring sites serve
        as the primary data foundation, supplemented by autonomous monitoring from LAS robotic platforms and IoT sensor
        networks. This comprehensive dataset spans {{ stats.date_range_years }} years ({{ stats.min_date }} to {{ stats.max_date }})
        and includes {{ stats.total_samples }} water samples with {{ stats.total_tests }} parameter measurements.
    </p>

    <h4>Data Import Methods</h4>

    <table class="data-table">
        <thead>
            <tr>
                <th>Method</th>
                <th>Source</th>
                <th>Frequency</th>
                <th>Parameters</th>
                <th>Status</th>
            </tr>
        </thead>
        <tbody>
            <tr style="background: #d4edda;">
                <td><strong>CSV Upload (Bulk Import)</strong></td>
                <td>Government agencies, research institutions, NGOs</td>
                <td>Batch imports (daily/weekly/monthly/quarterly)</td>
                <td>Full 45-parameter test results from accredited labs</td>
                <td><span class="badge bg-success">Primary Source</span></td>
            </tr>
            <tr style="background: #d4edda;">
                <td><strong>API Integration</strong></td>
                <td>Partner laboratories, government databases</td>
                <td>Daily/Weekly sync</td>
                <td>Standardized JSON format with WHO/BIS validation</td>
                <td><span class="badge bg-success">Active</span></td>
            </tr>
            <tr style="background: #e7f3ff;">
                <td><strong>Manual Entry</strong></td>
                <td>Field teams, community volunteers</td>
                <td>On-demand</td>
                <td>Field test kits (pH, turbidity, TDS, chlorine, coliform)</td>
                <td><span class="badge bg-info">Active</span></td>
            </tr>
            <tr style="background: #e7f3ff;">
                <td><strong>Mobile App</strong></td>
                <td>Field operators (manual sampling for lab analysis)</td>
                <td>Site visits</td>
                <td>GPS-tagged sample collection sent to laboratories</td>
                <td><span class="badge bg-info">Scaling Up</span></td>
            </tr>
            <tr style="background: #fff3cd;">
                <td><strong>LAS Autonomous Robot</strong></td>
                <td>Autonomous robotic platform on public water bodies</td>
                <td>Continuous autonomous monitoring</td>
                <td>Multi-sensor (pH, turbidity, DO, temperature, conductivity)</td>
                <td><span class="badge bg-warning text-dark">Scaling Up</span></td>
            </tr>
            <tr style="background: #fff3cd;">
                <td><strong>IoT Sensors</strong></td>
                <td>Inline sensors on residential water systems</td>
                <td>Real-time monitoring (15-min intervals)</td>
                <td>pH, turbidity, TDS, chlorine, temperature</td>
                <td><span class="badge bg-warning text-dark">Scaling Up</span></td>
            </tr>
        </tbody>
    </table>

    <h4>Data Quality and Validation</h4>

    <ul>
        <li>
            <strong>Laboratory Accreditation:</strong> All samples tested by NABL/ISO 17025 certified laboratories
            <div class="alert alert-info mt-2" style="font-size: 0.9em;">
                <strong>What is NABL/ISO 17025?</strong>
                <ul class="mb-0 mt-1">
                    <li>
                        <strong>NABL (National Accreditation Board for Testing and Calibration Laboratories):</strong>
                        India's premier accreditation body established by the Government of India. NABL accreditation ensures
                        that testing laboratories meet internationally recognized standards for technical competence, quality management,
                        and reliable testing practices. NABL operates under the Department of Science and Technology.
                    </li>
                    <li class="mt-2">
                        <strong>ISO/IEC 17025:</strong> International standard titled "General requirements for the competence
                        of testing and calibration laboratories." This standard specifies requirements for:
                        <ul class="mt-1">
                            <li>Technical competence of laboratory personnel</li>
                            <li>Validity and reliability of test results</li>
                            <li>Quality management systems</li>
                            <li>Proper equipment calibration and maintenance</li>
                            <li>Traceability of measurements to international standards</li>
                        </ul>
                    </li>
                    <li class="mt-2">
                        <strong>Examples of NABL-accredited water testing laboratories in India:</strong>
                        <ul class="mt-1">
                            <li>State Public Health Engineering Departments (PHED) laboratories</li>
                            <li>Central Pollution Control Board (CPCB) regional laboratories</li>
                            <li>State Pollution Control Board (SPCB) laboratories</li>
                            <li>Indian Institute of Technology (IIT) environmental testing labs</li>
                            <li>National Institute of Hydrology (NIH) laboratories</li>
                            <li>Private NABL-accredited laboratories (e.g., Eurofins, SGS India, TÜV SÜD)</li>
                            <li>Municipal corporation water quality testing facilities</li>
                        </ul>
                    </li>
                    <li class="mt-2">
                        <strong>Why NABL/ISO 17025 certification matters:</strong> Ensures that water quality test results
                        are accurate, reproducible, and legally defensible. Government agencies, courts, and regulatory bodies
                        accept test reports only from NABL/ISO 17025 accredited laboratories for compliance verification,
                        legal proceedings, and policy decisions.
                    </li>
                </ul>
            </div>
        </li>
        <li><strong>Parameter Standardization:</strong> Measurements converted to WHO/BIS standard units (mg/L, NTU, MPN/100mL)</li>
        <li><strong>Outlier Detection:</strong> Statistical validation removes measurement errors and sensor malfunctions</li>
        <li><strong>Geospatial Validation:</strong> GPS coordinates verified against known water body locations</li>
        <li><strong>Temporal Consistency:</strong> Time series analysis flags unrealistic parameter changes</li>
    </ul>

    <h4>Role of Data in System Intelligence</h4>

    <p>
        <strong>Bulk imported historical data serves as the primary training foundation</strong> for all ML models,
        supplemented by preliminary trial data from LAS and IoT systems (Phase 1).
        The 20,269+ labeled samples enable the system to:
    </p>

    <ul>
        <li>Learn contamination risk patterns across different site types and geographic regions</li>
        <li>Recognize chemical signatures of specific contamination sources (sewage, runoff, corrosion)</li>
        <li>Forecast seasonal water quality trends and predict threshold violations</li>
        <li>Optimize resource allocation by understanding detection-cost tradeoffs</li>
        <li>Detect both sudden anomalies and gradual drift in water quality parameters</li>
    </ul>

    <p>
        <strong>All 5 production-ready ML models are operational</strong>, trained on bulk imported historical laboratory data
        supplemented by autonomous sensor measurements from LAS robots and IoT networks. The system continues to import
        existing laboratory samples while integrating real-time autonomous sensor data for production predictions. The hybrid
        operational model balances high-reliability laboratory testing with high-frequency autonomous monitoring, weighted by
        data source reliability (laboratory > LAS > IoT) for decision-making.
    </p>

    <p class="mt-2">
        <em>See <a href="#section-12-1">Section 12.1 Implementation Roadmap</a> for detailed deployment timeline and scaling strategy.</em>
    </p>

    <h3 class="subsection-title" id="section-2-1">2.1 LAS: Autonomous Robotic Architecture</h3>

    <div class="alert alert-info" style="background: #e6f3ff; border-left: 4px solid #3182ce; padding: 15px; margin: 15px 0;">
        <strong><i class="bi bi-info-circle"></i> Development Status:</strong>
        LAS robotic platform is operational and scaling deployment across additional public water body
        monitoring sites. Real-time sensor data feeds into production ML models for site risk assessment and contamination
        detection. The architecture below represents validated specifications and planned enhancements for expanded deployment.
        <em>See <a href="#section-12-1">Section 12.1</a> for deployment roadmap.</em>
    </div>

    <p>
        <strong>LAS</strong> is a low-cost autonomous water quality assessment platform for continuous monitoring of
        public water bodies (stepwells, tanks, ponds, rivers). The platform integrates multi-parameter sensors with computer
        vision capabilities and GPS navigation, providing real-time data to production ML models for site risk assessment,
        contamination detection, and water quality forecasting.
    </p>

    <div class="row g-3 mb-3">
        <div class="col-md-4">
            <div class="card h-100">
                <div class="card-header bg-primary text-white">
                    <strong>Hardware Platform</strong>
                </div>
                <div class="card-body">
                    <ul class="small mb-0">
                        <li>Raspberry Pi based System</li>
                        <li>Fiberglass hull (1.2m × 0.8m × 0.4m)</li>
                        <li>Dual brushless DC motors (300W each)</li>
                        <li>GPS navigation (2.5m accuracy)</li>
                        <li>4G LTE real-time data upload</li>
                        <li>Solar charging (14-day autonomy)</li>
                    </ul>
                </div>
            </div>
        </div>
        <div class="col-md-4">
            <div class="card h-100">
                <div class="card-header bg-success text-white">
                    <strong>Sensor Suite</strong>
                </div>
                <div class="card-body">
                    <ul class="small mb-0">
                        <li>pH sensor (±0.1 accuracy)</li>
                        <li>TDS probe (±2% accuracy)</li>
                        <li>Turbidity sensor (0.1-1000 NTU)</li>
                        <li>Dissolved oxygen (±5%)</li>
                        <li>Temperature (±0.5°C)</li>
                        <li>5MP RGB camera for imaging</li>
                    </ul>
                </div>
            </div>
        </div>
    </div>

    <p>
        <strong>Cost Analysis:</strong> LAS reduces estimated per-test cost to Rs. 540 (amortized over
        3-year lifespan with 2 tests/week), a <strong>73% reduction</strong> compared to manual sampling (estimated Rs. 2,000/test
        including transport and labor).
    </p>

    <h4>Computer Vision for Algae Detection</h4>
    <p>
        LAS integrates a <strong>MobileNetV2-based computer vision pipeline</strong> for algae classification
        from RGB surface images. The system performs 4-class classification: No algae (clear water), Green algae
        (Chlorophyta), Cyanobacteria (blue-green, toxic), and Diatoms (brown/golden algae).
    </p>

    <table class="data-table">
        <thead>
            <tr>
                <th>Algae Class</th>
                <th>Precision</th>
                <th>Recall</th>
                <th>F1-Score</th>
                <th>Samples</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>No algae (clear water)</td>
                <td>0.91</td>
                <td>0.93</td>
                <td>0.92</td>
                <td>462</td>
            </tr>
            <tr>
                <td>Green algae (Chlorophyta)</td>
                <td>0.84</td>
                <td>0.82</td>
                <td>0.83</td>
                <td>122</td>
            </tr>
            <tr>
                <td>Cyanobacteria (toxic)</td>
                <td>0.88</td>
                <td>0.85</td>
                <td>0.86</td>
                <td>75</td>
            </tr>
            <tr>
                <td>Diatoms (brown/golden)</td>
                <td>0.79</td>
                <td>0.75</td>
                <td>0.77</td>
                <td>20</td>
            </tr>
            <tr style="background-color: #e6f7ff; font-weight: bold;">
                <td>Weighted Average</td>
                <td>0.88</td>
                <td>0.87</td>
                <td>0.87</td>
                <td>679</td>
            </tr>
        </tbody>
    </table>

    <p class="mt-2">
        <strong>Bacterial Contamination Estimation:</strong> Multi-modal regression combining turbidity,
        color intensity, and dissolved oxygen achieves <strong>R²=0.79 correlation with laboratory coliform tests</strong>
        (Pearson r=0.89, p&lt;0.001), enabling instant bacterial safety alerts without 24-48 hour laboratory delays.
    </p>

    <h3 class="subsection-title" id="section-2-2">2.2 IoT Sensor Architecture</h3>

    <div class="alert alert-info" style="background: #e6f3ff; border-left: 4px solid #3182ce; padding: 15px; margin: 15px 0;">
        <strong><i class="bi bi-info-circle"></i> Development Status:</strong>
        IoT inline sensor network is operational and scaling deployment across additional residential water systems.
        Real-time sensor data (15-minute intervals) feeds into production ML models for anomaly detection, contamination
        alerts, and water quality forecasting. The architecture below represents validated specifications and planned
        enhancements for expanded deployment. <em>See <a href="#section-12-1">Section 12.1</a> for deployment roadmap.</em>
    </div>

    <p>
        The IoT sensor network provides continuous real-time water quality monitoring for residential and small-scale
        water systems. Low-cost inline sensors (₹8,000-15,000 per installation) measure key parameters at 15-minute
        intervals, enabling immediate contamination alerts and real-time anomaly detection through integrated ML models.
        This architecture is designed for large-scale deployment across households and community water distribution systems.
    </p>

    <div class="row g-3 mb-3">
        <div class="col-md-4">
            <div class="card h-100">
                <div class="card-header bg-warning text-dark">
                    <strong>Hardware Components</strong>
                </div>
                <div class="card-body">
                    <ul class="small mb-0">
                        <li>Raspberry Pi or equivalent Microcontroller Based System</li>
                        <li>pH sensor</li>
                        <li>TDS sensor</li>
                        <li>Turbidity sensor</li>
                        <li>Temperature sensor</li>
                        <li>Chlorine sensor</li>
                        <li>IP65-rated enclosure</li>
                    </ul>
                </div>
            </div>
        </div>
        <div class="col-md-4">
            <div class="card h-100">
                <div class="card-header bg-secondary text-white">
                    <strong>Measurement Protocol</strong>
                </div>
                <div class="card-body">
                    <ul class="small mb-0">
                        <li>Sampling: 15-minute intervals (configurable 5-60 min)</li>
                        <li>Averaging: 3 measurements per cycle</li>
                        <li>Outlier rejection for data quality</li>
                        <li>Automated 2-point calibration every 7 days</li>
                        <li>12-hour battery backup during power outages</li>
                    </ul>
                </div>
            </div>
        </div>
        <div class="col-md-4">
            <div class="card h-100">
                <div class="card-header bg-dark text-white">
                    <strong>Data Pipeline</strong>
                </div>
                <div class="card-body">
                    <ul class="small mb-0">
                        <li><strong>Edge Computing:</strong> On-device WQI calculation (sub-100ms)</li>
                        <li>Cloud sync via MQTT</li>
                        <li>Time-series storage (InfluxDB, 90-day retention)</li>
                        <li>SMS/push notifications via SNS (sub-5-second latency)</li>
                    </ul>
                </div>
            </div>
        </div>
    </div>

    <p>
        <strong>Deployment Scope:</strong> The framework demonstrates viability for nationwide expansion supporting the
        multiple water supply initiatives and enabling global water quality collaboration across diverse regional water quality
        characteristics and environmental patterns.
    </p>

    <h3 class="subsection-title" id="section-2-3">2.3 Data Summary</h3>
    <div class="row g-3 mb-4">
        <div class="col-md-3">
            <div class="metric-card">
                <h4>Total Sites</h4>
                <div class="value">{{ stats.total_sites }}</div>
            </div>
        </div>
        <div class="col-md-3">
            <div class="metric-card" style="background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);">
                <h4>Water Samples</h4>
                <div class="value">{{ stats.total_samples }}</div>
            </div>
        </div>
        <div class="col-md-3">
            <div class="metric-card" style="background: linear-gradient(135deg, #fa709a 0%, #fee140 100%);">
                <h4>Date Range</h4>
                <div class="value">{{ stats.date_range_years }} yrs</div>
            </div>
        </div>
        <div class="col-md-3">
            <div class="metric-card" style="background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%); color: #333;">
                <h4>Samples/Site</h4>
                <div class="value">{{ stats.avg_samples_per_site }}</div>
            </div>
        </div>
    </div>

    <table class="data-table">
        <thead>
            <tr>
                <th>Metric</th>
                <th>Value</th>
                <th>Details</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>Sites Monitored</td>
                <td>{{ stats.total_sites }}</td>
                <td>{{ stats.states|join(', ') }}</td>
            </tr>
            <tr>
                <td>Water Samples</td>
                <td>{{ stats.total_samples }}</td>
                <td>{{ stats.min_date }} to {{ stats.max_date }}</td>
            </tr>
            <tr>
                <td>Test Results</td>
                <td>{{ stats.total_tests }}</td>
                <td>~{{ stats.avg_samples_per_site }} samples per site</td>
            </tr>
            <tr>
                <td>Site Risk Predictions</td>
                <td>{{ stats.risk_predictions }}</td>
                <td>Critical: {{ stats.risk_critical }}, High: {{ stats.risk_high }}, Medium: {{ stats.risk_medium }}, Low: {{ stats.risk_low }}</td>
            </tr>
            <tr>
                <td>Contamination Predictions</td>
                <td>{{ stats.contamination_predictions }}</td>
                <td>{{ stats.contamination_types|join(', ') }}</td>
            </tr>
            <tr>
                <td>WQI Readings</td>
                <td>{{ stats.wqi_readings }}</td>
                <td>Avg: {{ stats.avg_wqi }}, Classes: {{ stats.wqi_classes }}</td>
            </tr>
            <tr>
                <td>Water Quality Forecasts</td>
                <td>{{ stats.forecasts }}</td>
                <td>R²={{ stats.avg_r2 }} for pH, TDS, turbidity, temperature</td>
            </tr>
            <tr>
                <td>Cost Optimization Results</td>
                <td>{{ stats.cost_optimizations }}</td>
                <td>{{ stats.avg_cost_reduction }}% avg reduction, {{ stats.avg_detection_rate }}% detection</td>
            </tr>
        </tbody>
    </table>

    <h3 class="subsection-title" id="section-2-4">2.4 Water Quality Parameters</h3>
    <p>
        The Jal Sarovar system measures a comprehensive set of <strong>40+ water quality parameters</strong> across physical,
        chemical, and microbiological categories. These parameters are captured in laboratory test results and analyzed by
        the ML pipeline to assess water safety and contamination patterns.
    </p>

    <div class="alert alert-info mb-3">
        <strong><i class="bi bi-info-circle"></i> Key Parameters for WQI Calculation:</strong>
        The Water Quality Index (WQI) calculation prioritizes five key parameters: <strong>pH, TDS, Turbidity, Free Chlorine,
        and Total Coliform</strong>. A minimum of 3 key parameters is required for reliable WQI assessment.
    </div>

    <table class="data-table">
        <thead>
            <tr>
                <th>Category</th>
                <th>Parameter</th>
                <th>Unit</th>
                <th>Description</th>
            </tr>
        </thead>
        <tbody>
            <!-- Physical Parameters -->
            <tr style="background-color: #e6f2ff;">
                <td rowspan="7"><strong>Physical Parameters</strong></td>
                <td>pH</td>
                <td>-</td>
                <td>Acidity/alkalinity measure (optimal: 6.5-8.5)</td>
            </tr>
            <tr>
                <td>Temperature</td>
                <td>°C</td>
                <td>Water temperature in Celsius</td>
            </tr>
            <tr>
                <td>Turbidity</td>
                <td>NTU</td>
                <td>Cloudiness or haziness (threshold: &lt;5 NTU)</td>
            </tr>
            <tr>
                <td>Color</td>
                <td>Hazen</td>
                <td>Water color intensity</td>
            </tr>
            <tr>
                <td>Odor</td>
                <td>1-5 scale</td>
                <td>Odor threshold rating</td>
            </tr>
            <tr>
                <td>Taste</td>
                <td>1-5 scale</td>
                <td>Taste quality rating</td>
            </tr>
            <tr>
                <td>Conductivity</td>
                <td>µS/cm</td>
                <td>Electrical conductivity (microsiemens per centimeter)</td>
            </tr>

            <!-- Chemical Parameters - General -->
            <tr style="background-color: #fff4e6;">
                <td rowspan="5"><strong>Chemical - General</strong></td>
                <td>TDS (Total Dissolved Solids)</td>
                <td>ppm</td>
                <td>Total dissolved mineral content (threshold: &lt;500 ppm)</td>
            </tr>
            <tr>
                <td>Total Hardness</td>
                <td>mg/L</td>
                <td>Calcium and magnesium concentration</td>
            </tr>
            <tr>
                <td>Calcium Hardness</td>
                <td>mg/L</td>
                <td>Calcium-specific hardness</td>
            </tr>
            <tr>
                <td>Magnesium Hardness</td>
                <td>mg/L</td>
                <td>Magnesium-specific hardness</td>
            </tr>
            <tr>
                <td>Total Alkalinity</td>
                <td>mg/L</td>
                <td>Buffer capacity against pH changes</td>
            </tr>

            <!-- Disinfection Parameters -->
            <tr style="background-color: #e8f5e9;">
                <td rowspan="3"><strong>Disinfection</strong></td>
                <td>Free Chlorine</td>
                <td>mg/L</td>
                <td>Available chlorine for disinfection (optimal: 0.2-5.0 mg/L)</td>
            </tr>
            <tr>
                <td>Total Chlorine</td>
                <td>mg/L</td>
                <td>Combined free and bound chlorine</td>
            </tr>
            <tr>
                <td>Chlorine Residual</td>
                <td>mg/L</td>
                <td>Residual chlorine after reaction</td>
            </tr>

            <!-- Anions -->
            <tr style="background-color: #fce4ec;">
                <td rowspan="6"><strong>Anions</strong></td>
                <td>Chloride</td>
                <td>mg/L</td>
                <td>Chloride ion concentration (threshold: &lt;250 mg/L)</td>
            </tr>
            <tr>
                <td>Fluoride</td>
                <td>mg/L</td>
                <td>Fluoride content (optimal: 0.5-1.5 mg/L)</td>
            </tr>
            <tr>
                <td>Sulfate</td>
                <td>mg/L</td>
                <td>Sulfate concentration</td>
            </tr>
            <tr>
                <td>Nitrate</td>
                <td>mg/L</td>
                <td>Nitrate-nitrogen (threshold: &lt;50 mg/L)</td>
            </tr>
            <tr>
                <td>Nitrite</td>
                <td>mg/L</td>
                <td>Nitrite-nitrogen concentration</td>
            </tr>
            <tr>
                <td>Phosphate</td>
                <td>mg/L</td>
                <td>Phosphate concentration</td>
            </tr>

            <!-- Cations / Metals -->
            <tr style="background-color: #f3e5f5;">
                <td rowspan="13"><strong>Cations / Metals</strong></td>
                <td>Iron</td>
                <td>mg/L</td>
                <td>Iron content (threshold: &lt;0.3 mg/L)</td>
            </tr>
            <tr>
                <td>Manganese</td>
                <td>mg/L</td>
                <td>Manganese concentration (threshold: &lt;0.1 mg/L)</td>
            </tr>
            <tr>
                <td>Copper</td>
                <td>mg/L</td>
                <td>Copper content</td>
            </tr>
            <tr>
                <td>Zinc</td>
                <td>mg/L</td>
                <td>Zinc concentration</td>
            </tr>
            <tr>
                <td>Lead</td>
                <td>mg/L</td>
                <td>Lead content (threshold: &lt;0.01 mg/L)</td>
            </tr>
            <tr>
                <td>Arsenic</td>
                <td>mg/L</td>
                <td>Arsenic concentration (threshold: &lt;0.01 mg/L)</td>
            </tr>
            <tr>
                <td>Chromium</td>
                <td>mg/L</td>
                <td>Chromium content</td>
            </tr>
            <tr>
                <td>Cadmium</td>
                <td>mg/L</td>
                <td>Cadmium concentration (threshold: &lt;0.003 mg/L)</td>
            </tr>
            <tr>
                <td>Mercury</td>
                <td>mg/L</td>
                <td>Mercury content (threshold: &lt;0.001 mg/L)</td>
            </tr>
            <tr>
                <td>Nickel</td>
                <td>mg/L</td>
                <td>Nickel concentration</td>
            </tr>
            <tr>
                <td>Aluminum</td>
                <td>mg/L</td>
                <td>Aluminum content</td>
            </tr>
            <tr>
                <td>Sodium</td>
                <td>mg/L</td>
                <td>Sodium ion concentration</td>
            </tr>
            <tr>
                <td>Potassium</td>
                <td>mg/L</td>
                <td>Potassium content</td>
            </tr>

            <!-- Nitrogen Compounds -->
            <tr style="background-color: #e0f2f1;">
                <td rowspan="3"><strong>Nitrogen Compounds</strong></td>
                <td>Ammonia</td>
                <td>mg/L</td>
                <td>Ammonia-nitrogen (threshold: &lt;0.5 mg/L)</td>
            </tr>
            <tr>
                <td>Total Nitrogen</td>
                <td>mg/L</td>
                <td>All forms of nitrogen</td>
            </tr>
            <tr>
                <td>Organic Nitrogen</td>
                <td>mg/L</td>
                <td>Nitrogen in organic compounds</td>
            </tr>

            <!-- Organic Parameters -->
            <tr style="background-color: #fff3e0;">
                <td rowspan="4"><strong>Organic Parameters</strong></td>
                <td>Dissolved Oxygen (DO)</td>
                <td>mg/L</td>
                <td>Oxygen dissolved in water</td>
            </tr>
            <tr>
                <td>BOD (Biochemical Oxygen Demand)</td>
                <td>mg/L</td>
                <td>Oxygen required by bacteria to decompose organic matter</td>
            </tr>
            <tr>
                <td>COD (Chemical Oxygen Demand)</td>
                <td>mg/L</td>
                <td>Oxygen required to chemically oxidize organic matter</td>
            </tr>
            <tr>
                <td>TOC (Total Organic Carbon)</td>
                <td>mg/L</td>
                <td>Total organic carbon content</td>
            </tr>

            <!-- Microbiological Parameters -->
            <tr style="background-color: #ffebee;">
                <td rowspan="4"><strong>Microbiological</strong></td>
                <td>Total Coliform</td>
                <td>MPN/100mL</td>
                <td>Total coliform bacteria (should be 0 for drinking water)</td>
            </tr>
            <tr>
                <td>Fecal Coliform</td>
                <td>MPN/100mL</td>
                <td>Fecal contamination indicator</td>
            </tr>
            <tr>
                <td>E. coli</td>
                <td>MPN/100mL</td>
                <td>Escherichia coli presence (should be 0)</td>
            </tr>
            <tr>
                <td>Total Plate Count</td>
                <td>CFU/mL</td>
                <td>Total viable bacteria count (colony forming units)</td>
            </tr>

            <!-- Pesticides -->
            <tr style="background-color: #f1f8e9;">
                <td rowspan="2"><strong>Pesticides/Herbicides</strong></td>
                <td>Pesticides Detected</td>
                <td>Boolean</td>
                <td>Presence/absence of pesticides</td>
            </tr>
            <tr>
                <td>Pesticide Types</td>
                <td>List</td>
                <td>Specific pesticides identified</td>
            </tr>
        </tbody>
    </table>

    <p class="mt-3">
        <strong>Parameter Coverage Assessment:</strong> The system evaluates data quality based on parameter coverage.
        <em>Full assessment</em> requires ≥3 key parameters; <em>partial assessment</em> requires ≥1 key parameter;
        <em>insufficient data</em> indicates no key parameters measured. This tiered approach ensures ML predictions
        are only generated when sufficient data quality is available.
    </p>

    <h3 class="subsection-title" id="section-2-2-ml">2.2 ML Pipeline Architecture</h3>
    <div class="flowchart mb-4">
        <div class="flowchart-step">Sample Collection</div>
        <span class="flowchart-arrow">&rarr;</span>
        <div class="flowchart-step">Test Results</div>
        <span class="flowchart-arrow">&rarr;</span>
        <div class="flowchart-step">ML Analysis</div>
        <span class="flowchart-arrow">&rarr;</span>
        <div class="flowchart-step">Predictions</div>
        <span class="flowchart-arrow">&rarr;</span>
        <div class="flowchart-step">Alerts</div>
    </div>

    <div class="page-break"></div>

    <div class="alert alert-success" style="background: #d4edda; border-left: 4px solid #28a745; padding: 15px; margin: 20px 0;">
        <h4 style="margin-top: 0;">Machine Learning Models (5 Total - Data-Driven Algorithms)</h4>
        <p>The following models use <strong>machine learning</strong> algorithms that learn from data and optimize their behavior.</p>
        <ul style="margin-bottom: 0;">
            <li><strong>Site Risk Classifier:</strong> Random Forest (ensemble of decision trees for classification)</li>
            <li><strong>Contamination Classifier:</strong> XGBoost (gradient boosted trees for multi-class classification)</li>
            <li><strong>Water Quality Forecaster:</strong> Gaussian Process (Bayesian non-parametric regression with uncertainty quantification)</li>
            <li><strong>Bayesian Cost Optimizer:</strong> Bayesian Optimization (GP-based black-box optimization with Expected Improvement)</li>
            <li><strong>Hybrid Anomaly Detector:</strong> Isolation Forest + CUSUM (unsupervised anomaly detection with drift detection)</li>
        </ul>
    </div>

    <div class="alert alert-warning" style="background: #fff3cd; border-left: 4px solid #ffc107; padding: 15px; margin: 20px 0;">
        <h4 style="margin-top: 0;">Deterministic Algorithm (Rule-Based - NOT Machine Learning)</h4>
        <p>The following is <strong>NOT machine learning</strong>. It uses fixed formulas and thresholds defined by WHO/BIS standards. Same inputs always produce the same output.</p>
        <ul style="margin-bottom: 0;">
            <li><strong>Real-time WQI Calculator:</strong> Penalty-based scoring using WHO/BIS water quality standards (deterministic calculation)</li>
        </ul>
    </div>

    <!-- Site Risk Classifier -->
    <h2 class="section-title" id="section-3">3. Site Risk Classifier (Random Forest)</h2>

    <p><strong>Purpose:</strong> Predict contamination risk level for water bodies to prioritize testing resources.</p>
    <p><strong>Algorithm:</strong> Random Forest with feature engineering from site characteristics and historical data.</p>

    <h3 class="subsection-title" id="section-3-0">3.0 How It Functions</h3>

    <h4>Why Random Forest?</h4>
    <p>
        Random Forest was chosen for site risk classification because water contamination depends on <strong>complex interactions</strong> between multiple environmental factors. Unlike a single decision tree that might miss subtle patterns, Random Forest builds 100 independent trees, each learning from different data perspectives. This "wisdom of the crowd" approach handles the complexity of real-world contamination scenarios where industrial proximity might matter more for coastal sites than for inland agricultural areas.
    </p>
    <p>
        Think of training as teaching 100 different experts to recognize contamination risk. Each expert (decision tree) looks at the same historical data but focuses on different aspects. One expert might learn: "If a stepwell is near industry AND hasn't been tested in 60+ days, it's high risk." Another learns: "Coastal urban sites with 20%+ contamination history are critical." During training on 20,269 historical site assessments, the forest discovers that recent contamination history (42% importance) and testing recency (23% importance) are the strongest signals.
    </p>

    <h4>Prediction Workflow</h4>
    <p>
        When evaluating a new site, each of the 100 trees votes for a risk category (critical, high, medium, or low). The forest counts votes: if 73 trees vote "high risk" and 27 vote "medium risk," the site is classified as "high" with 73% confidence. The model then recommends bi-weekly testing (26 tests/year) for high-risk sites based on learned associations between risk levels and optimal monitoring frequencies.
    </p>

    <h4>Real-World Example</h4>
    <p>
        A stepwell near an industrial zone in Gujarat with 35% historical contamination rate and 45 days since last test would trigger majority votes for "high risk" because the model learned this combination often precedes contamination events. The system recommends bi-weekly testing (26 tests/year) to catch problems before they spread.
    </p>

    <h4>When Invoked</h4>
    <p>This model runs during <strong>testing schedule optimization</strong> to assign monitoring frequencies across all sites in the network.</p>

    <h3 class="subsection-title" id="section-3-1">3.1 Input Parameters</h3>
    <table class="data-table io-table">
        <thead>
            <tr>
                <th>Parameter</th>
                <th>Type</th>
                <th>Description</th>
                <th>Example</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>site_type</td><td>string</td><td>Type of water body</td><td>"stepwell", "tank", "pond", "lake"</td></tr>
            <tr><td>is_industrial_nearby</td><td>boolean</td><td>Industrial facilities within 5km</td><td>true/false</td></tr>
            <tr><td>is_agricultural_nearby</td><td>boolean</td><td>Agricultural land within 2km</td><td>true/false</td></tr>
            <tr><td>is_coastal</td><td>boolean</td><td>Within 10km of coastline</td><td>true/false</td></tr>
            <tr><td>is_urban</td><td>boolean</td><td>Urban population density &gt;500/km²</td><td>true/false</td></tr>
            <tr><td>contamination_rate_30d</td><td>float</td><td>Historical contamination % (30 days)</td><td>0-100</td></tr>
            <tr><td>days_since_last_test</td><td>int</td><td>Days since last water test</td><td>0-365</td></tr>
        </tbody>
    </table>

    <h3 class="subsection-title" id="section-3-2">3.2 Machine Learning Algorithm Details</h3>

    <p><strong>Implementation:</strong> sklearn.ensemble.RandomForestClassifier</p>
    <p><strong>Training Data:</strong> 20,269 labeled samples from historical site assessments</p>

    <h4>Hyperparameters</h4>
    <ul>
        <li><strong>n_estimators:</strong> 100 decision trees in the ensemble</li>
        <li><strong>max_depth:</strong> 8 levels per tree</li>
        <li><strong>min_samples_split:</strong> 10 samples required to split a node</li>
        <li><strong>class_weight:</strong> 'balanced' (handles class imbalance)</li>
        <li><strong>random_state:</strong> 42 (for reproducibility)</li>
    </ul>

    <h4>Feature Engineering</h4>
    <ul>
        <li><strong>One-hot encoding</strong> for site_type (4 binary features)</li>
        <li><strong>Boolean to binary:</strong> Environmental factors (0/1)</li>
        <li><strong>Numerical features:</strong> contamination_rate_30d, days_since_last_test</li>
        <li><strong>Total features:</strong> 10 input dimensions</li>
    </ul>

    <h4>Training Process</h4>
    <ol>
        <li>Split data: 80% training, 20% validation</li>
        <li>Build 100 decision trees on random subsamples</li>
        <li>Each tree learns different decision boundaries</li>
        <li>Ensemble aggregation: majority voting across all trees</li>
        <li>Calculate class probabilities from vote percentages</li>
    </ol>

    <h4>Model Performance</h4>
    <ul>
        <li><strong>Training Accuracy:</strong> 87.3%</li>
        <li><strong>Validation Accuracy:</strong> 84.6%</li>
        <li><strong>F1 Score:</strong> 0.835</li>
    </ul>

    <h4>Feature Importance (Learned from Data)</h4>
    <table class="data-table">
        <thead><tr><th>Feature</th><th>Importance</th><th>Interpretation</th></tr></thead>
        <tbody>
            <tr><td>contamination_rate_30d</td><td>0.42</td><td>Strongest predictor</td></tr>
            <tr><td>days_since_last_test</td><td>0.23</td><td>Testing recency matters</td></tr>
            <tr><td>is_industrial_nearby</td><td>0.15</td><td>Industrial proximity risk</td></tr>
            <tr><td>site_type_stepwell</td><td>0.12</td><td>Site-specific patterns</td></tr>
            <tr><td>is_urban</td><td>0.08</td><td>Urban contamination signal</td></tr>
        </tbody>
    </table>

    <h3 class="subsection-title" id="section-3-3">3.3 Output Parameters</h3>
    <table class="data-table io-table">
        <thead>
            <tr>
                <th>Parameter</th>
                <th>Type</th>
                <th>Description</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>risk_level</td><td>string</td><td>"critical", "high", "medium", "low"</td></tr>
            <tr><td>risk_score</td><td>float</td><td>0-100 numerical score</td></tr>
            <tr><td>confidence</td><td>float</td><td>Model confidence percentage</td></tr>
            <tr><td>recommended_frequency</td><td>string</td><td>"weekly", "bi-weekly", "monthly", "quarterly"</td></tr>
            <tr><td>tests_per_year</td><td>int</td><td>Recommended annual tests (52, 26, 12, or 4)</td></tr>
        </tbody>
    </table>

    <h3 class="subsection-title" id="section-3-4">3.4 Pseudocode</h3>
    <div class="pseudocode"><span class="keyword">FUNCTION</span> <span class="function">predict_site_risk_ml</span>(site_features):
    <span class="comment">// Load trained Random Forest model</span>
    model = LOAD_MODEL('site_risk_classifier.joblib')
    label_encoder = model.label_encoder

    <span class="comment">// Extract 10 features</span>
    features = [
        site_features.is_industrial_nearby,
        site_features.is_agricultural_nearby,
        site_features.is_coastal,
        site_features.is_urban,
        site_features.contamination_rate_30d,
        site_features.days_since_last_test,
        1 <span class="keyword">IF</span> site_type == 'stepwell' <span class="keyword">ELSE</span> 0,
        1 <span class="keyword">IF</span> site_type == 'tank' <span class="keyword">ELSE</span> 0,
        1 <span class="keyword">IF</span> site_type == 'pond' <span class="keyword">ELSE</span> 0,
        1 <span class="keyword">IF</span> site_type == 'lake' <span class="keyword">ELSE</span> 0
    ]

    <span class="comment">// Predict using 100 decision trees</span>
    class_probabilities = model.predict_proba([features])[0]
    predicted_class = model.predict([features])[0]

    <span class="comment">// Decode and map to frequency</span>
    risk_level = label_encoder.inverse_transform([predicted_class])[0]
    confidence = MAX(class_probabilities) × 100

    frequency_map = {
        'critical': ('weekly', 52),
        'high': ('bi-weekly', 26),
        'medium': ('monthly', 12),
        'low': ('quarterly', 4)
    }

    <span class="keyword">RETURN</span> {
        risk_level: risk_level,
        risk_score: confidence,
        confidence: confidence,
        prob_critical: class_probabilities[3],
        prob_high: class_probabilities[2],
        prob_medium: class_probabilities[1],
        prob_low: class_probabilities[0],
        recommended_frequency: frequency_map[risk_level][0],
        tests_per_year: frequency_map[risk_level][1]
    }</div>

    <div class="page-break"></div>

    <!-- Contamination Classifier -->
    <h2 class="section-title" id="section-4">4. Contamination Classifier (XGBoost)</h2>

    <p><strong>Purpose:</strong> Classify contamination source type to guide remediation strategies.</p>
    <p><strong>Algorithm:</strong> XGBoost multi-class classifier with probability outputs.</p>

    <h3 class="subsection-title" id="section-4-0">4.0 How It Functions</h3>

    <h4>Why Gradient Boosting?</h4>
    <p>
        XGBoost was selected for contamination classification because different pollution sources create <strong>overlapping chemical signatures</strong> that require progressive refinement. Sewage might elevate both coliform and ammonia, while pipe corrosion raises iron and manganese but not bacteria. Gradient boosting excels at these nuanced distinctions by building trees sequentially, where each new tree focuses on correcting the mistakes of previous trees—learning the subtle differences between contamination types.
    </p>
    <p>
        Imagine 100 increasingly specialized detectives investigating contamination. The first detective makes broad guesses: "High coliform = sewage." But this misses some cases. The second detective studies the first's errors: "High coliform WITHOUT elevated ammonia = agricultural runoff, not sewage." Each subsequent detective (tree) becomes an expert in the remaining confusing cases. By tree 100, the ensemble achieves 78.9% training accuracy on 20,508 labeled samples, with total_coliform (28% importance), turbidity (19%), and free_chlorine (17%) emerging as the strongest discriminators.
    </p>

    <h4>Prediction Workflow</h4>
    <p>
        For a new water sample, all 100 trees vote with weighted scores. Each tree applies learned decision rules like "if coliform > 15 AND ammonia > 0.8 → sewage_ingress (0.42 probability)." The final prediction aggregates these weighted votes through softmax conversion: sewage_ingress (65%), pipe_corrosion (22%), runoff_sediment (8%), others (5%). The model returns the top prediction (sewage) with 65% confidence.
    </p>

    <h4>Real-World Example</h4>
    <p>
        A residential water sample shows high turbidity (12 NTU), elevated coliform (25 MPN/100mL), high ammonia (1.2 mg/L), but normal iron levels. The model distinguishes this as <strong>sewage contamination</strong> (not pipe corrosion) because the ammonia-coliform combination without metallic signatures matches learned sewage patterns. This guides operators to inspect sewer lines rather than water pipes.
    </p>

    <h4>When Invoked</h4>
    <p>This model runs <strong>immediately after contamination detection</strong> (when WQI falls below thresholds) to identify the pollution source and guide targeted remediation.</p>

    <h3 class="subsection-title" id="section-4-1">4.1 Input Parameters</h3>
    <table class="data-table io-table">
        <thead>
            <tr>
                <th>Parameter</th>
                <th>Type</th>
                <th>Unit</th>
                <th>WHO/BIS Threshold</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>ph</td><td>float</td><td>-</td><td>6.5-8.5</td></tr>
            <tr><td>turbidity</td><td>float</td><td>NTU</td><td>&lt;5</td></tr>
            <tr><td>tds</td><td>float</td><td>ppm</td><td>&lt;500</td></tr>
            <tr><td>chlorine</td><td>float</td><td>mg/L</td><td>0.2-5.0</td></tr>
            <tr><td>iron</td><td>float</td><td>mg/L</td><td>&lt;0.3</td></tr>
            <tr><td>manganese</td><td>float</td><td>mg/L</td><td>&lt;0.4</td></tr>
            <tr><td>coliform</td><td>float</td><td>MPN/100mL</td><td>0</td></tr>
            <tr><td>ammonia</td><td>float</td><td>mg/L</td><td>&lt;0.5</td></tr>
            <tr><td>chloride</td><td>float</td><td>mg/L</td><td>&lt;250</td></tr>
        </tbody>
    </table>

    <h3 class="subsection-title" id="section-4-2">4.2 Machine Learning Algorithm Details</h3>

    <p><strong>Implementation:</strong> xgboost.XGBClassifier</p>
    <p><strong>Training Data:</strong> 20,508 labeled contamination samples</p>

    <h4>Hyperparameters</h4>
    <ul>
        <li><strong>n_estimators:</strong> 100 boosted trees</li>
        <li><strong>max_depth:</strong> 6 levels per tree</li>
        <li><strong>learning_rate:</strong> 0.1 (eta parameter)</li>
        <li><strong>subsample:</strong> 0.8 (80% data per tree)</li>
        <li><strong>colsample_bytree:</strong> 0.8 (80% features per tree)</li>
        <li><strong>objective:</strong> 'multi:softprob' (multi-class probabilities)</li>
    </ul>

    <h4>Feature Engineering</h4>
    <ul>
        <li><strong>Water quality parameters (9 features):</strong> pH, turbidity, TDS, free_chlorine, iron, manganese, total_coliform, ammonia, chloride</li>
        <li><strong>Environmental context (2 features):</strong> rained_recently, is_coastal</li>
        <li><strong>Feature scaling:</strong> StandardScaler normalization (mean=0, std=1)</li>
        <li><strong>Total features:</strong> 11 input dimensions</li>
    </ul>

    <h4>Training Process</h4>
    <ol>
        <li>Encode contamination types to integers (0-7)</li>
        <li>Gradient boosting iteration:
            <ul>
                <li>Fit each tree to residual errors of previous trees</li>
                <li>Each tree corrects ensemble mistakes</li>
                <li>Apply learning_rate to prevent overfitting</li>
                <li>Use subsample for randomness</li>
            </ul>
        </li>
        <li>Ensemble prediction: weighted sum of all 100 tree predictions</li>
        <li>Softmax conversion to probabilities</li>
    </ol>

    <h4>Model Performance</h4>
    <ul>
        <li><strong>Training Accuracy:</strong> 78.9%</li>
        <li><strong>Validation Accuracy:</strong> 73.4%</li>
        <li><strong>Macro F1 Score:</strong> 0.68</li>
        <li><strong>Best Detected:</strong> sewage_ingress (F1: 0.82)</li>
    </ul>

    <h4>Feature Importance (Learned via Gain)</h4>
    <table class="data-table">
        <thead><tr><th>Feature</th><th>Importance</th></tr></thead>
        <tbody>
            <tr><td>total_coliform</td><td>0.28</td></tr>
            <tr><td>turbidity</td><td>0.19</td></tr>
            <tr><td>free_chlorine</td><td>0.17</td></tr>
            <tr><td>TDS</td><td>0.12</td></tr>
            <tr><td>ammonia</td><td>0.10</td></tr>
            <tr><td>chloride</td><td>0.08</td></tr>
            <tr><td>pH</td><td>0.06</td></tr>
        </tbody>
    </table>

    <h3 class="subsection-title" id="section-4-3">4.3 Output Parameters</h3>
    <table class="data-table io-table">
        <thead>
            <tr>
                <th>Parameter</th>
                <th>Type</th>
                <th>Description</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>predicted_type</td><td>string</td><td>Contamination source type</td></tr>
            <tr><td>confidence</td><td>float</td><td>Probability × 100</td></tr>
            <tr><td>prob_runoff_sediment</td><td>float</td><td>0-1 probability</td></tr>
            <tr><td>prob_sewage_ingress</td><td>float</td><td>0-1 probability</td></tr>
            <tr><td>prob_salt_intrusion</td><td>float</td><td>0-1 probability</td></tr>
            <tr><td>prob_pipe_corrosion</td><td>float</td><td>0-1 probability</td></tr>
            <tr><td>prob_disinfectant_decay</td><td>float</td><td>0-1 probability</td></tr>
        </tbody>
    </table>

    <h3 class="subsection-title" id="section-4-4">4.4 Pseudocode</h3>
    <div class="pseudocode"><span class="keyword">FUNCTION</span> <span class="function">classify_contamination_ml</span>(test_result):
    <span class="comment">// Load trained XGBoost model</span>
    model = LOAD_MODEL('contamination_classifier.joblib')
    scaler = model.scaler
    label_encoder = model.label_encoder

    <span class="comment">// Extract 11 features</span>
    features = [
        test_result.ph,
        test_result.turbidity_ntu,
        test_result.tds_ppm,
        test_result.free_chlorine_mg_l,
        test_result.iron_mg_l,
        test_result.manganese_mg_l,
        test_result.total_coliform_mpn,
        test_result.ammonia_mg_l,
        test_result.chloride_mg_l,
        1 <span class="keyword">IF</span> test_result.rained_recently <span class="keyword">ELSE</span> 0,
        1 <span class="keyword">IF</span> test_result.is_coastal <span class="keyword">ELSE</span> 0
    ]

    <span class="comment">// Normalize features</span>
    features_scaled = scaler.transform([features])

    <span class="comment">// XGBoost prediction (100 boosted trees)</span>
    class_probabilities = model.predict_proba(features_scaled)[0]
    predicted_class_idx = model.predict(features_scaled)[0]

    <span class="comment">// Decode prediction</span>
    contamination_type = label_encoder.inverse_transform([predicted_class_idx])[0]
    confidence = MAX(class_probabilities) × 100

    <span class="keyword">RETURN</span> {
        predicted_type: contamination_type,
        confidence: confidence,
        prob_runoff_sediment: class_probabilities[0],
        prob_sewage_ingress: class_probabilities[1],
        prob_salt_intrusion: class_probabilities[2],
        prob_physical: class_probabilities[3],
        prob_chemical: class_probabilities[4],
        prob_bacterial: class_probabilities[5],
        prob_mixed: class_probabilities[6],
        prob_none: class_probabilities[7]
    }</div>

    <div class="page-break"></div>

    <!-- Water Quality Forecaster -->
    <h2 class="section-title" id="section-5">5. Water Quality Forecaster (Gaussian Process)</h2>

    <p><strong>Purpose:</strong> Predict future water quality parameter values with uncertainty bounds.</p>
    <p><strong>Algorithm:</strong> Gaussian Process regression with trend and seasonality components.</p>

    <h3 class="subsection-title" id="section-5-0">5.0 How It Functions</h3>

    <h4>Why Gaussian Process Instead of Linear Regression?</h4>
    <p>
        Water quality doesn't follow simple straight-line trends—pH might spike after monsoons, TDS increases gradually from salt intrusion, and turbidity shows seasonal patterns. Gaussian Processes (GP) excel here because they're <strong>non-parametric</strong>: instead of forcing data into a predetermined equation, GP learns the underlying pattern's shape from data. Crucially, GP provides <strong>Bayesian uncertainty quantification</strong>, telling operators "pH will be 7.2 ± 0.3" rather than just "7.2"—critical for proactive intervention planning.
    </p>
    <p>
        Think of Gaussian Processes as drawing a "cloud of possible futures" based on historical patterns. The model learns a <strong>similarity function</strong> (kernel): measurements close in time should have similar values. Training on historical time series [(Jan 1: pH 7.4), (Jan 15: pH 7.3), (Feb 1: pH 7.1)...], the GP discovers that pH typically changes smoothly over ~30-day periods (the learned length scale). The RBF kernel captures this: recent data points influence predictions more than distant ones, creating smooth, realistic forecasts.
    </p>

    <h4>Prediction Workflow</h4>
    <p>
        To forecast 90 days ahead, the GP computes similarity between the target date and all historical observations. It asks: "This future date is 45 days from our last reading—what did pH typically do after 45-day gaps historically?" The model outputs a probability distribution: mean prediction (most likely value) plus confidence bounds. For example: "30 days ahead, pH will be 7.3 ± 0.15 (95% confidence)" where uncertainty widens for distant predictions (90 days: ± 0.42).
    </p>

    <h4>Real-World Example</h4>
    <p>
        A coastal site shows TDS gradually increasing from 380 ppm (January) to 450 ppm (March). The GP forecast predicts TDS will reach 520 ppm by June (exceeding the 500 ppm threshold) with 82% confidence. This triggers <strong>proactive desalination planning</strong> rather than waiting for reactive contamination alerts—preventing 3 months of unsafe water.
    </p>

    <h4>When Invoked</h4>
    <p>This model runs <strong>weekly for monitored sites</strong> with sufficient historical data (minimum 10 samples) to generate 90-day forecasts for pH, TDS, and turbidity.</p>

    <h3 class="subsection-title" id="section-5-1">5.1 Input Parameters</h3>
    <table class="data-table io-table">
        <thead>
            <tr>
                <th>Parameter</th>
                <th>Type</th>
                <th>Description</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>site_id</td><td>int</td><td>Unique site identifier</td></tr>
            <tr><td>parameter</td><td>string</td><td>"ph", "turbidity", "tds", "chlorine", "temperature"</td></tr>
            <tr><td>historical_data</td><td>array</td><td>List of {date, value} pairs (minimum 10 samples)</td></tr>
            <tr><td>days_ahead</td><td>int</td><td>Forecast horizon (default: 90 days)</td></tr>
        </tbody>
    </table>

    <h3 class="subsection-title" id="section-5-2">5.2 Machine Learning Algorithm Details</h3>

    <p><strong>Implementation:</strong> sklearn.gaussian_process.GaussianProcessRegressor</p>
    <p><strong>Approach:</strong> Non-parametric Bayesian regression on time series</p>

    <h4>Kernel Function</h4>
    <p>RBF (Radial Basis Function) + White Noise:</p>
    <ul>
        <li><strong>Formula:</strong> k(x, x') = σ²_signal × exp(-||x - x'||² / (2 × l²)) + σ²_noise × δ(x, x')</li>
        <li><strong>σ²_signal:</strong> Signal variance (learned from data)</li>
        <li><strong>l:</strong> Length scale - determines smoothness (30 days)</li>
        <li><strong>σ²_noise:</strong> Noise variance - measurement uncertainty (0.1)</li>
    </ul>

    <h4>Hyperparameters</h4>
    <ul>
        <li><strong>Kernel:</strong> RBF(length_scale=30) + WhiteKernel(noise_level=0.1)</li>
        <li><strong>n_restarts_optimizer:</strong> 10 (avoid local optima)</li>
        <li><strong>alpha:</strong> 1e-10 (numerical stability)</li>
        <li><strong>normalize_y:</strong> True (zero-mean targets)</li>
    </ul>

    <h4>Training Process</h4>
    <ol>
        <li>Input: Historical time series [(t₁, y₁), (t₂, y₂), ..., (tₙ, yₙ)]</li>
        <li>Compute Gram matrix K: K_ij = k(tᵢ, tⱼ) for all observation pairs</li>
        <li>Maximum likelihood estimation: Optimize kernel hyperparameters θ</li>
        <li>Maximize log p(y | X, θ) = -½ yᵀK⁻¹y - ½ log|K| - n/2 log(2π)</li>
        <li>Store learned kernel parameters</li>
    </ol>

    <h4>Prediction Process (Bayesian Inference)</h4>
    <ol>
        <li>For new time t*:
            <ul>
                <li>Compute k* = [k(t*, t₁), k(t*, t₂), ..., k(t*, tₙ)]</li>
                <li>Compute k** = k(t*, t*)</li>
            </ul>
        </li>
        <li>Posterior mean (prediction): μ(t*) = k*ᵀ K⁻¹ y</li>
        <li>Posterior variance (uncertainty): σ²(t*) = k** - k*ᵀ K⁻¹ k*</li>
        <li>95% confidence interval: [μ(t*) - 1.96σ(t*), μ(t*) + 1.96σ(t*)]</li>
    </ol>

    <h4>Model Performance</h4>
    <ul>
        <li><strong>R² Score:</strong> 0.94 (94% variance explained)</li>
        <li><strong>MAE (pH):</strong> 0.12 units</li>
        <li><strong>MAE (TDS):</strong> 15.3 ppm</li>
        <li><strong>MAE (Turbidity):</strong> 0.8 NTU</li>
        <li><strong>MAE (Temperature):</strong> 1.2°C</li>
        <li><strong>Calibration:</strong> 95% of observations within 95% CI</li>
    </ul>

    <h4>Key Advantages</h4>
    <ul>
        <li>Uncertainty quantification (confidence intervals)</li>
        <li>Non-linear trend capture</li>
        <li>Automatic smoothness tuning</li>
        <li>Works well with small datasets (>10 samples)</li>
    </ul>

    <h3 class="subsection-title" id="section-5-3">5.3 Output Parameters</h3>
    <table class="data-table io-table">
        <thead>
            <tr>
                <th>Parameter</th>
                <th>Type</th>
                <th>Description</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>forecast_date</td><td>date</td><td>Date of prediction</td></tr>
            <tr><td>predicted_value</td><td>float</td><td>Point forecast</td></tr>
            <tr><td>lower_bound_95</td><td>float</td><td>95% confidence lower bound</td></tr>
            <tr><td>upper_bound_95</td><td>float</td><td>95% confidence upper bound</td></tr>
            <tr><td>uncertainty</td><td>float</td><td>Standard error of prediction</td></tr>
            <tr><td>prob_exceed_threshold</td><td>float</td><td>P(value > WHO threshold)</td></tr>
            <tr><td>r2_score</td><td>float</td><td>Model fit quality (0-1)</td></tr>
        </tbody>
    </table>

    <h3 class="subsection-title" id="section-5-4">5.4 Pseudocode</h3>
    <div class="pseudocode"><span class="keyword">FUNCTION</span> <span class="function">forecast_water_quality_gp</span>(site_id, parameter, historical_data, days_ahead):
    <span class="comment">// Extract historical time series</span>
    X_train = EXTRACT(historical_data, "days_since_epoch")  <span class="comment">// Time as input</span>
    y_train = EXTRACT(historical_data, "value")

    <span class="comment">// Define Gaussian Process kernel</span>
    kernel = RBF(length_scale=30) + WhiteKernel(noise_level=0.1)
    gp = GaussianProcessRegressor(
        kernel=kernel,
        n_restarts_optimizer=10,
        alpha=1e-10,
        normalize_y=<span class="keyword">TRUE</span>
    )

    <span class="comment">// Train GP on historical data</span>
    gp.fit(X_train, y_train)

    <span class="comment">// Generate forecast times</span>
    X_forecast = [TODAY + 1, TODAY + 2, ..., TODAY + days_ahead]

    <span class="comment">// Predict with uncertainty (Bayesian posterior)</span>
    μ_pred, σ_pred = gp.predict(X_forecast, return_std=<span class="keyword">TRUE</span>)

    <span class="comment">// Build forecast results</span>
    forecasts = []
    <span class="keyword">FOR</span> i = 0 <span class="keyword">TO</span> LENGTH(X_forecast):
        predicted_value = μ_pred[i]
        uncertainty = σ_pred[i]

        lower_95 = predicted_value - 1.96 × uncertainty
        upper_95 = predicted_value + 1.96 × uncertainty

        <span class="comment">// Calculate exceedance probability using normal CDF</span>
        threshold = GET_WHO_THRESHOLD(parameter)
        z_score = (threshold - predicted_value) / uncertainty
        prob_exceed = 1 - NORMAL_CDF(z_score)

        forecasts.APPEND({
            date: X_forecast[i],
            predicted: predicted_value,
            lower_bound: lower_95,
            upper_bound: upper_95,
            uncertainty: uncertainty,
            prob_exceed_threshold: prob_exceed
        })

    <span class="comment">// Calculate R² score for model quality</span>
    y_pred_train = gp.predict(X_train)
    r2 = 1 - SUM((y_train - y_pred_train)²) / SUM((y_train - MEAN(y_train))²)

    <span class="keyword">RETURN</span> {forecasts: forecasts, r2_score: r2, kernel_params: gp.kernel_}</div>

    <div class="page-break"></div>

    <!-- Bayesian Cost Optimizer -->
    <h2 class="section-title" id="section-6">6. Bayesian Cost Optimizer</h2>

    <p><strong>Purpose:</strong> Optimize testing resource allocation across sites to maximize contamination detection within budget constraints.</p>
    <p><strong>Algorithm:</strong> Bayesian Optimization with Gaussian Process surrogate model and Expected Improvement acquisition function.</p>
    <p><strong>Objective:</strong> Find optimal testing schedule that maximizes detection_rate while minimizing cost.</p>

    <h3 class="subsection-title" id="section-6-0">6.0 How It Functions</h3>

    <h4>Why Bayesian Optimization?</h4>
    <p>
        Finding optimal testing schedules across dozens of sites is a <strong>"black-box" optimization problem</strong>—evaluating each schedule requires expensive simulations of detection rates and costs, making brute-force search impractical (testing all combinations would require millions of evaluations). Bayesian Optimization excels by building a probabilistic model of the objective function, intelligently selecting which schedules to test next based on Expected Improvement, converging to near-optimal solutions in just 50 iterations instead of millions.
    </p>

    <h4>How Expected Improvement Works in Simple Terms</h4>
    <p>
        Expected Improvement (EI) balances exploration and exploitation like a smart treasure hunter. Current best schedule: ₹2.3M cost, 88% detection rate. Should we try testing high-risk sites weekly (expensive but thorough) or reduce all sites to quarterly (cheap but risky)? EI computes: "Weekly high-risk testing has 40% chance of improving detection to 92% (big gain but uncertain), while quarterly reduction has 80% chance of small 1% improvement (safe but limited)." The optimizer picks the option with highest expected benefit, gradually learning which schedule patterns work best.
    </p>

    <h4>Learning and Optimization Loop</h4>
    <p>
        The optimizer starts by testing 10 random schedules to understand the landscape. Each evaluation simulates: "If we test Site A weekly (52×₹1000) and Site B monthly (12×₹1000), what's the combined detection rate?" After each test, a Gaussian Process model learns: "Weekly testing of high-risk coastal sites gives better detection-per-rupee than weekly testing low-risk inland sites." The acquisition function (EI) then suggests: "Next, try bi-weekly for medium-risk sites—high potential for cost savings." This cycle repeats for 50 iterations, converging to schedules achieving 35-40% cost reduction while maintaining 85%+ detection rates.
    </p>

    <h4>Real-World Example</h4>
    <p>
        Current baseline: All 50 sites tested monthly = 600 annual tests × ₹1,000 = ₹6,00,000/year. The optimizer discovers: "Critical sites (5) tested bi-weekly (130 tests), high-risk (15) monthly (180 tests), medium (20) bi-monthly (120 tests), low (10) quarterly (40 tests) = 470 total tests = ₹4,70,000 (22% savings) with 86% detection rate vs. baseline 84%." It found this by learning that recent contamination history predicts future events better than geographic proximity.
    </p>

    <h4>When Invoked</h4>
    <p>This model runs <strong>quarterly during budget planning</strong> to optimize testing resource allocation across the entire site network based on updated risk assessments.</p>

    <h3 class="subsection-title" id="section-6-1">6.1 Input Parameters</h3>
    <table class="data-table io-table">
        <thead>
            <tr>
                <th>Parameter</th>
                <th>Type</th>
                <th>Description</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>sites</td><td>array</td><td>List of site objects with risk_score</td></tr>
            <tr><td>budget_inr</td><td>float</td><td>Available annual budget in INR</td></tr>
            <tr><td>cost_per_test</td><td>float</td><td>Estimated cost per water test (default: ₹1000)</td></tr>
        </tbody>
    </table>

    <h3 class="subsection-title" id="section-6-2">6.2 Machine Learning Algorithm Details</h3>

    <p><strong>Implementation:</strong> Bayesian Optimization with sklearn.gaussian_process.GaussianProcessRegressor</p>
    <p><strong>Problem Type:</strong> Black-box optimization (expensive-to-evaluate objective function)</p>

    <h4>Search Space</h4>
    <p>Optimize testing frequency for each site:</p>
    <ul>
        <li><strong>Decision variables:</strong> tests_per_year for each of N sites</li>
        <li><strong>Domain:</strong> Each variable ∈ {4, 6, 12, 26, 52} (discrete choices: quarterly, bi-monthly, monthly, bi-weekly, weekly)</li>
        <li><strong>Dimensionality:</strong> N-dimensional optimization (one frequency per site)</li>
    </ul>

    <h4>Objective Function</h4>
    <p>Multi-objective optimization with scalarization:</p>
    <ul>
        <li><strong>f(x) = α × detection_rate(x) - β × normalized_cost(x)</strong></li>
        <li><strong>α = 0.7:</strong> Weight for detection rate maximization</li>
        <li><strong>β = 0.3:</strong> Weight for cost minimization</li>
        <li><strong>detection_rate(x):</strong> Weighted average across sites (higher risk sites weighted more)</li>
        <li><strong>normalized_cost(x):</strong> Total cost divided by budget</li>
        <li><strong>Goal:</strong> Maximize f(x) subject to total_cost(x) ≤ budget</li>
    </ul>

    <h4>Gaussian Process Surrogate Model</h4>
    <ul>
        <li><strong>Kernel:</strong> Matérn(ν=2.5) - captures non-smooth objective landscape</li>
        <li><strong>Length scale:</strong> Auto-tuned via maximum likelihood estimation</li>
        <li><strong>Noise level:</strong> 0.01 (small noise due to deterministic simulation)</li>
        <li><strong>Purpose:</strong> Build probabilistic model of f(x) from observed evaluations</li>
    </ul>

    <h4>Acquisition Function</h4>
    <p><strong>Expected Improvement (EI)</strong> with exploration-exploitation tradeoff:</p>
    <ul>
        <li><strong>Formula:</strong> EI(x) = E[max(0, f(x) - f(x*)] where x* is current best</li>
        <li><strong>Interpretation:</strong> Expected improvement over current best solution</li>
        <li><strong>ξ parameter:</strong> 0.01 (exploration bonus)</li>
        <li><strong>Selection:</strong> x_next = argmax EI(x) over candidate points</li>
    </ul>

    <h4>Bayesian Optimization Loop</h4>
    <ol>
        <li><strong>Initialize:</strong> Evaluate 10 random schedules (Latin Hypercube Sampling)</li>
        <li><strong>Build GP:</strong> Fit Gaussian Process to observed {(x₁, f(x₁)), ..., (xₙ, f(xₙ))}</li>
        <li><strong>Acquisition:</strong> Find x_next = argmax EI(x) using gradient-free optimization</li>
        <li><strong>Evaluate:</strong> Simulate schedule x_next → compute detection_rate and cost → get f(x_next)</li>
        <li><strong>Update:</strong> Add (x_next, f(x_next)) to observations</li>
        <li><strong>Repeat:</strong> Steps 2-5 for 50 iterations or until convergence</li>
    </ol>

    <h4>Convergence Criteria</h4>
    <ul>
        <li>Maximum iterations: 50</li>
        <li>Early stopping: If best f(x) doesn't improve by >0.01 for 10 consecutive iterations</li>
        <li>Budget constraint: total_cost(x_best) ≤ budget_inr</li>
    </ul>

    <h4>Hyperparameters</h4>
    <ul>
        <li><strong>n_initial:</strong> 10 (random initialization points)</li>
        <li><strong>n_iter:</strong> 50 (maximum optimization iterations)</li>
        <li><strong>kernel:</strong> Matérn(nu=2.5, length_scale=1.0, length_scale_bounds=(1e-2, 1e2))</li>
        <li><strong>acquisition:</strong> Expected Improvement (EI)</li>
        <li><strong>xi:</strong> 0.01 (exploration parameter)</li>
        <li><strong>random_state:</strong> 42 (for reproducibility)</li>
    </ul>

    <h4>Key Advantages of Bayesian Optimization</h4>
    <ul>
        <li>Efficient for expensive black-box functions (each schedule simulation requires computing site-level detection rates)</li>
        <li>Uncertainty quantification via GP posterior variance</li>
        <li>Automatic exploration-exploitation balance</li>
        <li>Global optimization (not stuck in local optima like gradient descent)</li>
        <li>Works with discrete/mixed search spaces</li>
    </ul>

    <h3 class="subsection-title" id="section-6-3">6.3 Output Parameters</h3>
    <table class="data-table io-table">
        <thead>
            <tr>
                <th>Parameter</th>
                <th>Type</th>
                <th>Description</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>total_current_cost</td><td>float</td><td>Baseline annual cost (INR)</td></tr>
            <tr><td>total_optimized_cost</td><td>float</td><td>Optimized annual cost (INR)</td></tr>
            <tr><td>total_savings</td><td>float</td><td>Annual savings (INR)</td></tr>
            <tr><td>cost_reduction_percent</td><td>float</td><td>Percentage reduction</td></tr>
            <tr><td>average_detection_rate</td><td>float</td><td>Expected contamination detection %</td></tr>
        </tbody>
    </table>

    <h3 class="subsection-title" id="section-6-4">6.4 Pseudocode</h3>
    <div class="pseudocode"><span class="keyword">FUNCTION</span> <span class="function">bayesian_optimize_schedule</span>(sites, budget, cost_per_test):
    <span class="comment">// Initialize search space</span>
    n_sites = LENGTH(sites)
    frequency_choices = [4, 6, 12, 26, 52]  <span class="comment">// Tests per year</span>

    <span class="comment">// Define objective function (black-box, expensive to evaluate)</span>
    <span class="keyword">FUNCTION</span> <span class="function">objective</span>(schedule):
        total_cost = 0
        weighted_detection = 0
        total_weight = 0

        <span class="keyword">FOR</span> i = 0 <span class="keyword">TO</span> n_sites:
            tests_per_year = schedule[i]
            cost = tests_per_year × cost_per_test
            total_cost += cost

            <span class="comment">// Detection rate model (learned from historical data)</span>
            detection_rate = MIN(99, 70 + (tests_per_year / 52) × 30)

            <span class="comment">// Weight by risk score (high-risk sites matter more)</span>
            weight = sites[i].risk_score / 100
            weighted_detection += detection_rate × weight
            total_weight += weight

        <span class="keyword">IF</span> total_cost > budget <span class="keyword">THEN RETURN</span> -∞  <span class="comment">// Constraint violation</span>

        avg_detection = weighted_detection / total_weight
        normalized_cost = total_cost / budget

        <span class="comment">// Scalarized objective: maximize detection, minimize cost</span>
        <span class="keyword">RETURN</span> 0.7 × avg_detection - 0.3 × normalized_cost × 100

    <span class="comment">// Phase 1: Random initialization (Latin Hypercube Sampling)</span>
    X_observed = []  <span class="comment">// Evaluated schedules</span>
    y_observed = []  <span class="comment">// Objective values</span>

    <span class="keyword">FOR</span> i = 1 <span class="keyword">TO</span> 10:
        schedule = RANDOM_SAMPLE(frequency_choices, size=n_sites)
        score = objective(schedule)
        X_observed.APPEND(schedule)
        y_observed.APPEND(score)

    best_score = MAX(y_observed)
    best_schedule = X_observed[ARGMAX(y_observed)]

    <span class="comment">// Phase 2: Bayesian Optimization loop</span>
    <span class="keyword">FOR</span> iteration = 1 <span class="keyword">TO</span> 50:
        <span class="comment">// Fit Gaussian Process surrogate model</span>
        kernel = Matérn(nu=2.5, length_scale=1.0)
        gp = GaussianProcessRegressor(kernel=kernel, alpha=0.01)
        gp.fit(X_observed, y_observed)

        <span class="comment">// Define Expected Improvement acquisition function</span>
        <span class="keyword">FUNCTION</span> <span class="function">expected_improvement</span>(x_candidate):
            μ, σ = gp.predict([x_candidate], return_std=<span class="keyword">TRUE</span>)

            <span class="keyword">IF</span> σ == 0 <span class="keyword">THEN RETURN</span> 0

            Z = (μ - best_score - 0.01) / σ  <span class="comment">// ξ = 0.01</span>
            EI = (μ - best_score - 0.01) × NORMAL_CDF(Z) + σ × NORMAL_PDF(Z)
            <span class="keyword">RETURN</span> EI

        <span class="comment">// Find next point to evaluate by maximizing EI</span>
        best_EI = -∞
        x_next = <span class="keyword">NULL</span>

        <span class="comment">// Search over candidate points (grid or random sampling)</span>
        <span class="keyword">FOR</span> candidate <span class="keyword">IN</span> GENERATE_CANDIDATES(frequency_choices, n_sites):
            ei = expected_improvement(candidate)
            <span class="keyword">IF</span> ei > best_EI <span class="keyword">THEN</span>
                best_EI = ei
                x_next = candidate

        <span class="comment">// Evaluate objective at selected point</span>
        y_next = objective(x_next)

        <span class="comment">// Update observations</span>
        X_observed.APPEND(x_next)
        y_observed.APPEND(y_next)

        <span class="comment">// Update best found solution</span>
        <span class="keyword">IF</span> y_next > best_score <span class="keyword">THEN</span>
            best_score = y_next
            best_schedule = x_next
            no_improvement_count = 0
        <span class="keyword">ELSE</span>
            no_improvement_count += 1

        <span class="comment">// Early stopping</span>
        <span class="keyword">IF</span> no_improvement_count >= 10 <span class="keyword">THEN BREAK</span>

    <span class="comment">// Convert best schedule to results</span>
    results = []
    <span class="keyword">FOR</span> i = 0 <span class="keyword">TO</span> n_sites:
        results.APPEND({
            site_id: sites[i].id,
            site_name: sites[i].name,
            optimized_tests_per_year: best_schedule[i],
            optimized_cost: best_schedule[i] × cost_per_test
        })

    <span class="keyword">RETURN</span> {
        optimal_schedule: results,
        objective_value: best_score,
        iterations: LENGTH(X_observed),
        model_version: 'bayesian_v1'
    }</div>

    <div class="page-break"></div>

    <!-- Real-time WQI Calculator -->
    <h2 class="section-title" id="section-7">7. Real-time WQI Calculator (Rule-Based)</h2>

    <div class="alert alert-info" style="background: #e6f3ff; border-left: 4px solid #3182ce; padding: 15px; margin: 15px 0;">
        <strong>⚠️ Note:</strong> This is <strong>NOT a machine learning model</strong>.
        It is a <strong>deterministic calculation</strong> based on WHO/BIS water quality standards.
        The WQI score is computed using a fixed penalty-based formula with no training data or learning involved.
    </div>

    <p><strong>Purpose:</strong> Calculate Water Quality Index score for real-time assessment.</p>
    <p><strong>Algorithm Type:</strong> Rules-based penalty scoring system (not ML)</p>
    <p><strong>Standards:</strong> Aligned with WHO Guidelines for Drinking-water Quality and BIS 10500:2012</p>

    <h3 class="subsection-title" id="section-7-1">7.1 Input Parameters</h3>
    <table class="data-table io-table">
        <thead>
            <tr>
                <th>Parameter</th>
                <th>Type</th>
                <th>Unit</th>
                <th>Optimal Range</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>ph</td><td>float</td><td>-</td><td>6.5 - 8.5</td></tr>
            <tr><td>tds</td><td>float</td><td>ppm</td><td>&lt; 500</td></tr>
            <tr><td>turbidity</td><td>float</td><td>NTU</td><td>&lt; 5</td></tr>
            <tr><td>chlorine</td><td>float</td><td>mg/L</td><td>0.2 - 5.0</td></tr>
            <tr><td>temperature</td><td>float</td><td>°C</td><td>10 - 25</td></tr>
        </tbody>
    </table>

    <h3 class="subsection-title" id="section-7-2">7.2 Internal Operations (Penalty Scoring)</h3>
    <ol>
        <li>Start with perfect score: WQI = 100</li>
        <li><strong>pH Penalty (max 20):</strong>
            <ul>
                <li>If pH &lt; 6.5: penalty = min(20, (6.5 - pH) × 10)</li>
                <li>If pH &gt; 8.5: penalty = min(20, (pH - 8.5) × 10)</li>
            </ul>
        </li>
        <li><strong>TDS Penalty (max 30):</strong> If TDS &gt; 500: penalty = min(30, (TDS - 500) / 50)</li>
        <li><strong>Turbidity Penalty (max 20):</strong> If turbidity &gt; 5: penalty = min(20, (turbidity - 5) × 2)</li>
        <li><strong>Chlorine Penalty (max 15):</strong> If chlorine &lt; 0.2: penalty = 15; If chlorine &gt; 5.0: penalty = 10</li>
        <li><strong>Temperature Penalty (max 10):</strong> If outside 10-25°C range</li>
        <li>Final WQI = clamp(100 - sum(penalties), 0, 100)</li>
        <li>Classify: Excellent (≥90), Compliant (≥70), Warning (≥50), Unsafe (&lt;50)</li>
    </ol>

    <h3 class="subsection-title" id="section-7-3">7.3 Output Parameters</h3>
    <table class="data-table io-table">
        <thead>
            <tr>
                <th>Parameter</th>
                <th>Type</th>
                <th>Description</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>wqi_score</td><td>float</td><td>0-100 composite score</td></tr>
            <tr><td>wqi_class</td><td>string</td><td>"Excellent", "Compliant", "Warning", "Unsafe"</td></tr>
            <tr><td>is_drinkable</td><td>boolean</td><td>True if WQI ≥ 70</td></tr>
            <tr><td>ph_penalty</td><td>float</td><td>pH contribution to score reduction</td></tr>
            <tr><td>tds_penalty</td><td>float</td><td>TDS contribution</td></tr>
            <tr><td>turbidity_penalty</td><td>float</td><td>Turbidity contribution</td></tr>
            <tr><td>chlorine_penalty</td><td>float</td><td>Chlorine contribution</td></tr>
            <tr><td>temperature_penalty</td><td>float</td><td>Temperature contribution</td></tr>
        </tbody>
    </table>

    <h3 class="subsection-title" id="section-7-4">7.4 Pseudocode</h3>
    <div class="pseudocode"><span class="keyword">FUNCTION</span> <span class="function">calculate_wqi</span>(sensor_reading):
    wqi = 100
    penalties = {}

    <span class="comment">// pH penalty (max 20 points)</span>
    <span class="keyword">IF</span> ph < 6.5 <span class="keyword">THEN</span> penalties.ph = MIN(20, (6.5 - ph) × 10)
    <span class="keyword">ELSE IF</span> ph > 8.5 <span class="keyword">THEN</span> penalties.ph = MIN(20, (ph - 8.5) × 10)
    <span class="keyword">ELSE</span> penalties.ph = 0

    <span class="comment">// TDS penalty (max 30 points)</span>
    <span class="keyword">IF</span> tds > 500 <span class="keyword">THEN</span> penalties.tds = MIN(30, (tds - 500) / 50)
    <span class="keyword">ELSE</span> penalties.tds = 0

    <span class="comment">// Turbidity penalty (max 20 points)</span>
    <span class="keyword">IF</span> turbidity > 5 <span class="keyword">THEN</span> penalties.turbidity = MIN(20, (turbidity - 5) × 2)
    <span class="keyword">ELSE</span> penalties.turbidity = 0

    <span class="comment">// Chlorine penalty (max 15 points)</span>
    <span class="keyword">IF</span> chlorine < 0.2 <span class="keyword">THEN</span> penalties.chlorine = 15
    <span class="keyword">ELSE IF</span> chlorine > 5.0 <span class="keyword">THEN</span> penalties.chlorine = 10
    <span class="keyword">ELSE</span> penalties.chlorine = 0

    <span class="comment">// Calculate final WQI</span>
    wqi = 100 - SUM(penalties.values())
    wqi = CLAMP(wqi, 0, 100)

    <span class="comment">// Classify</span>
    <span class="keyword">IF</span> wqi >= 90 <span class="keyword">THEN</span> class = "Excellent"; drinkable = TRUE
    <span class="keyword">ELSE IF</span> wqi >= 70 <span class="keyword">THEN</span> class = "Compliant"; drinkable = TRUE
    <span class="keyword">ELSE IF</span> wqi >= 50 <span class="keyword">THEN</span> class = "Warning"; drinkable = FALSE
    <span class="keyword">ELSE</span> class = "Unsafe"; drinkable = FALSE

    <span class="keyword">RETURN</span> {score: wqi, class: class, drinkable: drinkable, penalties: penalties}</div>

    <div class="page-break"></div>

    <!-- Hybrid Anomaly Detection -->
    <h2 class="section-title" id="section-8">8. Hybrid Anomaly Detection (Isolation Forest + CUSUM)</h2>

    <p><strong>Purpose:</strong> Detect unusual sensor readings indicating equipment failure, contamination events, or data quality issues through two complementary approaches.</p>
    <p><strong>Algorithm:</strong> Hybrid system combining (1) Isolation Forest for sudden anomalies and (2) CUSUM (Cumulative Sum) for gradual drift detection.</p>
    <p><strong>Performance:</strong> 92% accuracy with 12-minute average detection delay for residential monitoring (time from anomaly occurrence to alert delivery).</p>

    <div class="alert alert-info mb-3" style="background: #ebf8ff; border-left: 4px solid #4299e1; padding: 12px 15px; font-size: 0.9rem;">
        <strong>Detection Delay Breakdown (Residential Monitoring):</strong>
        <ul style="margin-bottom: 0; margin-top: 5px;">
            <li><strong>Sampling interval:</strong> 15-minute readings (average 7.5 min wait for next measurement)</li>
            <li><strong>Edge processing:</strong> 42 milliseconds (on-device WQI calculation)</li>
            <li><strong>Cloud upload:</strong> 2.3 seconds (MQTT transmission)</li>
            <li><strong>Alert delivery:</strong> 1.8 seconds (SMS/push notification via SNS)</li>
            <li><strong>Total average delay:</strong> ~12 minutes (dominated by sampling interval)</li>
        </ul>
        <em>Note: Sampling interval can be configured to 5 minutes (reducing delay to ~4.2 minutes) at the cost of 27% higher power consumption.</em>
    </div>

    <h3 class="subsection-title" id="section-8-1">8.1 Part A: Isolation Forest - Sudden Anomaly Detection</h3>
    <p><strong>Purpose:</strong> Detect sudden spikes or drops in water quality parameters (e.g., contamination events, sensor failures).</p>

    <h4 style="font-size: 1rem; color: #4a5568; margin-top: 15px; margin-bottom: 10px;">Input Parameters</h4>
    <table class="data-table io-table">
        <thead>
            <tr>
                <th>Parameter</th>
                <th>Type</th>
                <th>Description</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>current_reading</td><td>dict</td><td>Current sensor values {ph, tds, turbidity, chlorine, temperature}</td></tr>
            <tr><td>historical_stats</td><td>dict</td><td>Per-parameter statistics {mean, std} from historical data</td></tr>
        </tbody>
    </table>

    <h4 style="font-size: 1rem; color: #4a5568; margin-top: 15px; margin-bottom: 10px;">Machine Learning Algorithm Details</h4>

    <p><strong>Implementation:</strong> sklearn.ensemble.IsolationForest</p>
    <p><strong>Training Data:</strong> 15,000+ normal water quality samples (contamination-free baseline)</p>

    <h4>Hyperparameters</h4>
    <ul>
        <li><strong>n_estimators:</strong> 100 isolation trees</li>
        <li><strong>contamination:</strong> 0.1 (expected 10% anomaly rate)</li>
        <li><strong>max_samples:</strong> 'auto' (256 samples per tree)</li>
        <li><strong>random_state:</strong> 42 (for reproducibility)</li>
    </ul>

    <h4>How Isolation Forest Works</h4>
    <ol>
        <li><strong>Tree Construction:</strong> Build 100 binary trees by randomly selecting:
            <ul>
                <li>A feature (pH, TDS, turbidity, chlorine, or temperature)</li>
                <li>A split value between min and max of that feature</li>
            </ul>
        </li>
        <li><strong>Path Length Calculation:</strong> For each sample:
            <ul>
                <li>Traverse each tree until reaching a leaf node</li>
                <li>Count steps (path length) to isolation</li>
                <li>Anomalies isolate quickly (short paths)</li>
                <li>Normal samples require more splits (long paths)</li>
            </ul>
        </li>
        <li><strong>Anomaly Score:</strong> Average path length across all 100 trees
            <ul>
                <li>Score ∈ [0, 1] where higher = more anomalous</li>
                <li>Score &gt; 0.5 typically indicates anomaly</li>
            </ul>
        </li>
    </ol>

    <h4>Key Advantages</h4>
    <ul>
        <li>Unsupervised learning (no labeled anomalies needed)</li>
        <li>Handles multivariate anomalies (unusual combinations of parameters)</li>
        <li>Fast prediction (O(log n) per tree)</li>
        <li>Robust to feature scaling differences</li>
    </ul>

    <h4 style="font-size: 1rem; color: #4a5568; margin-top: 15px; margin-bottom: 10px;">Output Parameters</h4>
    <table class="data-table io-table">
        <thead>
            <tr>
                <th>Parameter</th>
                <th>Type</th>
                <th>Description</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>is_anomaly</td><td>boolean</td><td>True if any parameter exceeds 3σ</td></tr>
            <tr><td>anomaly_type</td><td>string</td><td>"spike" or "drop"</td></tr>
            <tr><td>anomaly_score</td><td>float</td><td>0-1 severity score</td></tr>
            <tr><td>parameter</td><td>string</td><td>Most anomalous parameter</td></tr>
            <tr><td>deviation_sigma</td><td>float</td><td>Number of standard deviations</td></tr>
            <tr><td>details</td><td>array</td><td>Per-parameter anomaly details</td></tr>
        </tbody>
    </table>

    <h4 style="font-size: 1rem; color: #4a5568; margin-top: 15px; margin-bottom: 10px;">Pseudocode - Isolation Forest</h4>
    <div class="pseudocode"><span class="keyword">FUNCTION</span> <span class="function">detect_anomaly_iforest</span>(current_reading):
    <span class="comment">// Load trained Isolation Forest model</span>
    model = LOAD_MODEL('anomaly_detector.joblib')
    scaler = model.scaler

    <span class="comment">// Extract features (5 water quality parameters)</span>
    features = [
        current_reading.ph,
        current_reading.turbidity_ntu,
        current_reading.tds_ppm,
        current_reading.free_chlorine_mg_l,
        current_reading.temperature_c
    ]

    <span class="comment">// Normalize features (same scaling as training)</span>
    features_scaled = scaler.transform([features])

    <span class="comment">// Predict anomaly using Isolation Forest</span>
    <span class="comment">// Returns: -1 for anomaly, +1 for normal</span>
    prediction = model.predict(features_scaled)[0]
    is_anomaly = (prediction == -1)

    <span class="comment">// Get anomaly score (0 to 1, higher = more anomalous)</span>
    <span class="comment">// Based on average path length across 100 trees</span>
    raw_score = model.decision_function(features_scaled)[0]
    <span class="comment">// Normalize: raw_score ranges from ~-0.5 (anomaly) to ~0.5 (normal)</span>
    normalized_score = CLIP(-raw_score, 0, 1)

    <span class="comment">// Determine which parameter is most anomalous</span>
    <span class="comment">// By checking which causes largest score change when removed</span>
    anomaly_param = FIND_MOST_INFLUENTIAL_FEATURE(model, features_scaled)

    <span class="keyword">RETURN</span> {
        is_anomaly: is_anomaly,
        anomaly_score: normalized_score,
        anomaly_type: "spike" <span class="keyword">IF</span> is_anomaly <span class="keyword">ELSE</span> NULL,
        parameter: anomaly_param,
        confidence: normalized_score,
        model_version: 'iforest_v1'
    }</div>

    <h3 class="subsection-title" id="section-8-2">8.2 Part B: CUSUM - Gradual Drift Detection</h3>
    <p><strong>Purpose:</strong> Detect gradual degradation or drift in water quality parameters over time (e.g., pipe corrosion, membrane degradation, infrastructure aging).</p>
    <p><strong>Method:</strong> CUSUM (Cumulative Sum Control Chart) tracks cumulative deviations from expected baseline to identify sustained shifts.</p>

    <h4 style="font-size: 1rem; color: #4a5568; margin-top: 15px; margin-bottom: 10px;">Input Parameters</h4>
    <table class="data-table io-table">
        <thead>
            <tr>
                <th>Parameter</th>
                <th>Type</th>
                <th>Description</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>measurement_sequence</td><td>array</td><td>Time-ordered sequence of sensor readings</td></tr>
            <tr><td>baseline_mean</td><td>float</td><td>Expected parameter value (from training period)</td></tr>
            <tr><td>threshold</td><td>float</td><td>CUSUM threshold (default: 5σ)</td></tr>
            <tr><td>drift_magnitude</td><td>float</td><td>Minimum meaningful shift to detect (default: 0.5σ)</td></tr>
        </tbody>
    </table>

    <h4 style="font-size: 1rem; color: #4a5568; margin-top: 15px; margin-bottom: 10px;">Internal Operations - CUSUM Algorithm</h4>
    <p>CUSUM maintains two cumulative sums to detect upward (S<sub>H</sub>) and downward (S<sub>L</sub>) shifts:</p>
    <ol>
        <li>For each new measurement x<sup>(t)</sup>:
            <ul>
                <li>Standardize: z<sup>(t)</sup> = (x<sup>(t)</sup> - μ) / σ</li>
                <li>Update upward CUSUM: S<sub>H</sub><sup>(t)</sup> = max(0, S<sub>H</sub><sup>(t-1)</sup> + z<sup>(t)</sup> - k)</li>
                <li>Update downward CUSUM: S<sub>L</sub><sup>(t)</sup> = max(0, S<sub>L</sub><sup>(t-1)</sup> - z<sup>(t)</sup> - k)</li>
                <li>Slack parameter k = 0.5 (half-sigma shift detection)</li>
            </ul>
        </li>
        <li>Drift detected if S<sub>H</sub> > threshold OR S<sub>L</sub> > threshold</li>
        <li>Reset cumulative sums after drift detection and intervention</li>
    </ol>

    <h4 style="font-size: 1rem; color: #4a5568; margin-top: 15px; margin-bottom: 10px;">Output Parameters</h4>
    <table class="data-table io-table">
        <thead>
            <tr>
                <th>Parameter</th>
                <th>Type</th>
                <th>Description</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>drift_detected</td><td>boolean</td><td>True if gradual drift identified</td></tr>
            <tr><td>drift_direction</td><td>string</td><td>"upward" or "downward"</td></tr>
            <tr><td>cusum_value</td><td>float</td><td>Current cumulative sum magnitude</td></tr>
            <tr><td>parameter</td><td>string</td><td>Parameter showing drift</td></tr>
            <tr><td>drift_start_time</td><td>datetime</td><td>Estimated drift onset timestamp</td></tr>
        </tbody>
    </table>

    <h4 style="font-size: 1rem; color: #4a5568; margin-top: 15px; margin-bottom: 10px;">Pseudocode - CUSUM Drift Detection</h4>
    <div class="pseudocode"><span class="keyword">FUNCTION</span> <span class="function">detect_drift_cusum</span>(measurements, baseline_mean, baseline_std, threshold=5.0):
    k = 0.5  <span class="comment">// Slack parameter (half-sigma shift)</span>
    S_H = 0  <span class="comment">// Cumulative sum for upward drift</span>
    S_L = 0  <span class="comment">// Cumulative sum for downward drift</span>
    drift_detected = FALSE
    drift_start = NULL

    <span class="keyword">FOR</span> each measurement x <span class="keyword">IN</span> measurements:
        <span class="comment">// Standardize measurement</span>
        z = (x - baseline_mean) / baseline_std

        <span class="comment">// Update CUSUM statistics</span>
        S_H = MAX(0, S_H + z - k)
        S_L = MAX(0, S_L - z - k)

        <span class="comment">// Check for drift</span>
        <span class="keyword">IF</span> S_H > threshold <span class="keyword">THEN</span>
            drift_detected = TRUE
            drift_direction = "upward"
            cusum_value = S_H
            <span class="keyword">IF</span> drift_start IS NULL <span class="keyword">THEN</span> drift_start = TIMESTAMP(x)

        <span class="keyword">ELSE IF</span> S_L > threshold <span class="keyword">THEN</span>
            drift_detected = TRUE
            drift_direction = "downward"
            cusum_value = S_L
            <span class="keyword">IF</span> drift_start IS NULL <span class="keyword">THEN</span> drift_start = TIMESTAMP(x)

    <span class="keyword">RETURN</span> {
        drift_detected: drift_detected,
        direction: drift_direction,
        cusum_value: cusum_value,
        drift_start_time: drift_start
    }</div>

    <h3 class="subsection-title" id="section-8-0">8.0 Why Use TWO Detectors Together?</h3>

    <h4>Complementary Detection Capabilities</h4>
    <p>
        Isolation Forest and CUSUM catch <strong>fundamentally different anomaly types</strong>. Imagine water quality as a patient's vital signs: Isolation Forest is like a smoke alarm that detects sudden heart attacks (spike: pH drops from 7.5 to 5.2 overnight due to contamination event), while CUSUM is like tracking gradual weight gain over months (drift: TDS slowly increasing from 380 to 510 ppm over 60 days due to pipe corrosion).
    </p>

    <h4>Real-World Detection Examples</h4>
    <p>
        <strong>Sudden Anomaly (Isolation Forest catches it):</strong> A sewage line ruptures near a municipal water tank. Coliform spikes from 0 to 45 MPN/100mL within 4 hours. Isolation Forest detects this immediately because the reading is far outside normal multivariate patterns (short path length in isolation trees), triggering emergency alerts.
    </p>
    <p>
        <strong>Gradual Drift (CUSUM catches it):</strong> Membrane filters in a residential RO system slowly degrade over 90 days. TDS creeps from 380 → 420 → 465 → 510 ppm across weekly measurements. Each individual reading seems "normal-ish" so Isolation Forest doesn't trigger, but CUSUM accumulates these small deviations (cumulative sum exceeds threshold after 60 days), alerting operators to schedule filter replacement <strong>before</strong> TDS exceeds the 500 ppm safety limit.
    </p>

    <h4>When Each Detector Activates</h4>
    <p>
        <strong>Isolation Forest:</strong> Runs on <strong>every sensor reading</strong> (15-minute intervals for IoT sensors, per-test for LAS lab samples) to catch sudden contamination, sensor malfunctions, or data transmission errors.
    </p>
    <p>
        <strong>CUSUM:</strong> Runs on <strong>time-series windows</strong> (trailing 30-90 days of data) to detect infrastructure aging, seasonal drift, or gradual source contamination that develops slowly enough to evade sudden anomaly detection.
    </p>

    <h3 class="subsection-title" id="section-8-3">8.3 Hybrid Detection Strategy</h3>
    <p>The system runs <strong>both</strong> Isolation Forest and CUSUM concurrently:</p>
    <ul>
        <li><strong>Isolation Forest:</strong> Triggers immediate alerts for sudden anomalies (contamination events, sensor failures)</li>
        <li><strong>CUSUM:</strong> Triggers maintenance alerts for gradual drift (pipe corrosion, membrane degradation)</li>
        <li><strong>Combined Performance:</strong> 92% accuracy, 12-minute average detection delay for residential monitoring (time from anomaly occurrence until alert is sent to homeowner)</li>
        <li><strong>False Positive Rate:</strong> 8% (filtered via confirmation logic requiring 2 consecutive detections)</li>
        <li><strong>Detection Delay Composition:</strong> Primarily limited by 15-minute sampling interval; actual computation and alert delivery occurs in under 5 seconds</li>
    </ul>

    <div class="page-break"></div>

    <!-- Overall Pipeline -->
    <h2 class="section-title" id="section-9">9. Complete ML Analysis Pipeline</h2>

    <p>The following pseudocode describes the complete end-to-end <strong>machine learning pipeline</strong> that runs when a new water sample is processed. This shows actual ML model operations including loading trained models, feature preprocessing, and ML inference:</p>

    <div class="pseudocode"><span class="keyword">FUNCTION</span> <span class="function">run_ml_analysis_pipeline</span>(site, sample, test_result):
    <span class="comment">// ========== STEP 1: Site Risk Classifier (Random Forest) ==========</span>
    <span class="comment">// Load trained Random Forest model (100 trees, trained on 20,269 samples)</span>
    rf_model = LOAD_MODEL('ml/models/site_risk_classifier.joblib')
    rf_label_encoder = rf_model.label_encoder

    <span class="comment">// Feature extraction (10 features with one-hot encoding)</span>
    site_features = [
        site.is_industrial_nearby,           <span class="comment">// Boolean → 0/1</span>
        site.is_agricultural_nearby,
        site.is_coastal,
        site.is_urban,
        CALCULATE_CONTAMINATION_RATE(site, days=30),  <span class="comment">// Numerical</span>
        DAYS_SINCE_LAST_TEST(site),
        1 <span class="keyword">IF</span> site.type == 'stepwell' <span class="keyword">ELSE</span> 0,  <span class="comment">// One-hot encoded</span>
        1 <span class="keyword">IF</span> site.type == 'tank' <span class="keyword">ELSE</span> 0,
        1 <span class="keyword">IF</span> site.type == 'pond' <span class="keyword">ELSE</span> 0,
        1 <span class="keyword">IF</span> site.type == 'lake' <span class="keyword">ELSE</span> 0
    ]

    <span class="comment">// Random Forest inference: 100 trees vote</span>
    class_probabilities = rf_model.predict_proba([site_features])[0]
    predicted_class = rf_model.predict([site_features])[0]
    risk_level = rf_label_encoder.inverse_transform([predicted_class])[0]
    confidence = MAX(class_probabilities) × 100

    risk_result = {
        risk_level: risk_level,  <span class="comment">// 'critical', 'high', 'medium', 'low'</span>
        confidence: confidence,
        probabilities: class_probabilities
    }
    STORE(site_risk_predictions, risk_result)

    <span class="comment">// ========== STEP 2: Contamination Classifier (XGBoost) ==========</span>
    <span class="keyword">IF</span> test_result IS NOT NULL <span class="keyword">THEN</span>
        <span class="comment">// Load trained XGBoost model (100 boosted trees, trained on 20,508 samples)</span>
        xgb_model = LOAD_MODEL('ml/models/contamination_classifier.joblib')
        xgb_scaler = xgb_model.scaler
        xgb_label_encoder = xgb_model.label_encoder

        <span class="comment">// Feature extraction (11 features)</span>
        contamination_features = [
            test_result.ph,
            test_result.turbidity_ntu,
            test_result.tds_ppm,
            test_result.free_chlorine_mg_l,
            test_result.iron_mg_l,
            test_result.manganese_mg_l,
            test_result.total_coliform_mpn,
            test_result.ammonia_mg_l,
            test_result.chloride_mg_l,
            1 <span class="keyword">IF</span> sample.rained_recently <span class="keyword">ELSE</span> 0,
            1 <span class="keyword">IF</span> site.is_coastal <span class="keyword">ELSE</span> 0
        ]

        <span class="comment">// Feature scaling (StandardScaler: mean=0, std=1)</span>
        contamination_features_scaled = xgb_scaler.transform([contamination_features])

        <span class="comment">// XGBoost inference: 100 boosted trees with gradient boosting</span>
        contamination_probs = xgb_model.predict_proba(contamination_features_scaled)[0]
        contamination_class = xgb_model.predict(contamination_features_scaled)[0]
        contamination_type = xgb_label_encoder.inverse_transform([contamination_class])[0]

        contamination_result = {
            predicted_type: contamination_type,
            confidence: MAX(contamination_probs) × 100,
            probabilities: {
                runoff_sediment: contamination_probs[0],
                sewage_ingress: contamination_probs[1],
                salt_intrusion: contamination_probs[2],
                physical: contamination_probs[3],
                chemical: contamination_probs[4],
                bacterial: contamination_probs[5],
                mixed: contamination_probs[6],
                none: contamination_probs[7]
            }
        }
        STORE(contamination_predictions, contamination_result)

    <span class="comment">// ========== STEP 3: WQI Calculator (Rule-Based - NOT ML) ==========</span>
    wqi_score = 100
    penalty = 0

    <span class="comment">// Rule-based penalty scoring (no ML involved)</span>
    <span class="keyword">IF</span> test_result.ph < 6.5 OR test_result.ph > 8.5 <span class="keyword">THEN</span> penalty += 10
    <span class="keyword">IF</span> test_result.turbidity_ntu > 5 <span class="keyword">THEN</span> penalty += 15
    <span class="keyword">IF</span> test_result.total_coliform_mpn > 0 <span class="keyword">THEN</span> penalty += 30
    <span class="comment">// ... additional penalty rules ...</span>

    wqi_score = MAX(0, 100 - penalty)
    wqi_result = {score: wqi_score, grade: GET_WQI_GRADE(wqi_score)}
    STORE(wqi_readings, wqi_result)

    <span class="comment">// ========== STEP 4: Hybrid Anomaly Detector (Isolation Forest + CUSUM) ==========</span>
    <span class="comment">// Load trained Isolation Forest model (100 trees, trained on 15,000+ normal samples)</span>
    iforest_model = LOAD_MODEL('ml/models/anomaly_detector.joblib')
    iforest_scaler = iforest_model.scaler

    <span class="comment">// Feature extraction (5 water quality parameters)</span>
    anomaly_features = [
        test_result.ph,
        test_result.turbidity_ntu,
        test_result.tds_ppm,
        test_result.free_chlorine_mg_l,
        test_result.temperature_c
    ]

    <span class="comment">// Feature scaling</span>
    anomaly_features_scaled = iforest_scaler.transform([anomaly_features])

    <span class="comment">// Isolation Forest inference: average path length across 100 trees</span>
    prediction = iforest_model.predict(anomaly_features_scaled)[0]  <span class="comment">// -1 or +1</span>
    is_anomaly = (prediction == -1)

    <span class="comment">// Anomaly score based on average isolation path length</span>
    raw_score = iforest_model.decision_function(anomaly_features_scaled)[0]
    anomaly_score = CLIP(-raw_score, 0, 1)

    anomaly_result = {
        is_anomaly: is_anomaly,
        anomaly_score: anomaly_score,
        confidence: anomaly_score
    }
    STORE(anomaly_detections, anomaly_result)

    <span class="keyword">IF</span> is_anomaly <span class="keyword">THEN</span> TRIGGER_ALERT(site, anomaly_result)

    <span class="comment">// ========== STEP 5: Water Quality Forecaster (Gaussian Process) ==========</span>
    <span class="keyword">IF</span> COUNT(GET_HISTORICAL_SAMPLES(site.id)) >= 10 <span class="keyword">THEN</span>
        forecasts = []

        <span class="keyword">FOR</span> param <span class="keyword">IN</span> [ph, turbidity, tds, temperature]:
            <span class="comment">// Get historical time series</span>
            historical_data = GET_PARAMETER_HISTORY(site.id, param, days=90)
            X_train = [days_since_epoch for each reading]
            y_train = [parameter_value for each reading]

            <span class="comment">// Define Gaussian Process kernel</span>
            kernel = RBF(length_scale=30) + WhiteKernel(noise_level=0.1)
            gp = GaussianProcessRegressor(
                kernel=kernel,
                n_restarts_optimizer=10,
                alpha=1e-10,
                normalize_y=<span class="keyword">TRUE</span>
            )

            <span class="comment">// Train GP on historical data (fit GP to observations)</span>
            gp.fit(X_train, y_train)

            <span class="comment">// Generate 90-day forecast</span>
            X_forecast = [TODAY + i for i in 1..90]

            <span class="comment">// GP Bayesian inference: compute posterior mean and variance</span>
            μ_pred, σ_pred = gp.predict(X_forecast, return_std=<span class="keyword">TRUE</span>)

            <span class="comment">// Build forecast with uncertainty quantification</span>
            <span class="keyword">FOR</span> i = 0 <span class="keyword">TO</span> 90:
                forecasts.APPEND({
                    date: X_forecast[i],
                    parameter: param,
                    predicted_value: μ_pred[i],
                    lower_95: μ_pred[i] - 1.96 × σ_pred[i],
                    upper_95: μ_pred[i] + 1.96 × σ_pred[i],
                    uncertainty: σ_pred[i]
                })

        STORE(water_quality_forecasts, forecasts)

    <span class="comment">// ========== STEP 6: Bayesian Cost Optimizer ==========</span>
    <span class="comment">// Run periodically (e.g., monthly) for all sites</span>
    <span class="keyword">IF</span> IS_OPTIMIZATION_DAY() <span class="keyword">THEN</span>
        all_sites = GET_ALL_SITES_WITH_RISK_SCORES()

        <span class="comment">// Initialize Bayesian Optimization</span>
        X_observed = []  <span class="comment">// Evaluated schedules</span>
        y_observed = []  <span class="comment">// Objective function values</span>

        <span class="comment">// Phase 1: Random initialization (10 schedules)</span>
        <span class="keyword">FOR</span> i = 1 <span class="keyword">TO</span> 10:
            schedule = RANDOM_SCHEDULE(all_sites)
            score = EVALUATE_OBJECTIVE(schedule, all_sites, budget)
            X_observed.APPEND(schedule)
            y_observed.APPEND(score)

        best_schedule = X_observed[ARGMAX(y_observed)]

        <span class="comment">// Phase 2: Bayesian Optimization loop (up to 50 iterations)</span>
        <span class="keyword">FOR</span> iteration = 1 <span class="keyword">TO</span> 50:
            <span class="comment">// Fit Gaussian Process surrogate model</span>
            kernel = Matérn(nu=2.5, length_scale=1.0)
            gp = GaussianProcessRegressor(kernel=kernel, alpha=0.01)
            gp.fit(X_observed, y_observed)

            <span class="comment">// Find next schedule by maximizing Expected Improvement</span>
            x_next = ARGMAX_EXPECTED_IMPROVEMENT(gp, X_observed, y_observed)

            <span class="comment">// Evaluate objective function at selected schedule</span>
            y_next = EVALUATE_OBJECTIVE(x_next, all_sites, budget)

            <span class="comment">// Update observations</span>
            X_observed.APPEND(x_next)
            y_observed.APPEND(y_next)

            <span class="keyword">IF</span> y_next > MAX(y_observed) <span class="keyword">THEN</span>
                best_schedule = x_next

        optimization_result = {
            optimal_schedule: best_schedule,
            objective_value: MAX(y_observed),
            total_cost: CALCULATE_COST(best_schedule)
        }
        STORE(cost_optimization_results, optimization_result)

    <span class="comment">// ========== RETURN AGGREGATED ML RESULTS ==========</span>
    <span class="keyword">RETURN</span> {
        site_risk_ml: risk_result,              <span class="comment">// Random Forest output</span>
        contamination_ml: contamination_result,  <span class="comment">// XGBoost output</span>
        wqi_formula: wqi_result,                <span class="comment">// Not ML (deterministic)</span>
        anomaly_ml: anomaly_result,             <span class="comment">// Isolation Forest output</span>
        forecasts_ml: forecasts,                <span class="comment">// Gaussian Process output</span>
        optimization_ml: optimization_result     <span class="comment">// Bayesian Optimization output</span>
    }</div>

    <div class="page-break"></div>

    <!-- Intervention Management -->
    <h2 class="section-title" id="section-10">10. Intervention Management and Treatment Effectiveness</h2>

    <p>
        The Jal Sarovar system integrates an intervention management module that tracks water treatment actions,
        remediation efforts, and their effectiveness. This closed-loop system enables evidence-based decision-making
        by connecting contamination detection with treatment implementation and outcome measurement.
    </p>

    <h3 class="subsection-title" id="section-10-1">10.1 Intervention Framework</h3>
    <p>
        When the ML pipeline detects water quality issues through contamination classification or anomaly detection,
        the system automatically suggests appropriate interventions based on:
    </p>
    <ul>
        <li><strong>Contamination type:</strong> Different treatment methods for sewage ingress vs. salt intrusion vs. pipe corrosion</li>
        <li><strong>Severity level:</strong> Critical issues trigger immediate response protocols</li>
        <li><strong>Cost-effectiveness:</strong> Treatment recommendations balanced against available budget</li>
        <li><strong>Historical effectiveness:</strong> Past intervention success rates inform future recommendations</li>
    </ul>

    <h3 class="subsection-title" id="section-10-2">10.2 Intervention Types</h3>
    <p>The system tracks eight categories of interventions:</p>
    <table class="data-table">
        <thead>
            <tr>
                <th>Intervention Type</th>
                <th>Description</th>
                <th>Typical Applications</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>Water Treatment</td>
                <td>Chemical or physical treatment processes</td>
                <td>Coagulation, flocculation, sedimentation</td>
            </tr>
            <tr>
                <td>Cleaning/Maintenance</td>
                <td>Regular maintenance and cleaning operations</td>
                <td>Tank cleaning, sediment removal, biofilm treatment</td>
            </tr>
            <tr>
                <td>Infrastructure Repair</td>
                <td>Structural repairs to water infrastructure</td>
                <td>Leak repair, crack sealing, lining replacement</td>
            </tr>
            <tr>
                <td>Equipment Replacement</td>
                <td>Replacement of failed or degraded equipment</td>
                <td>Pump replacement, valve upgrade, filter media change</td>
            </tr>
            <tr>
                <td>Chlorination</td>
                <td>Disinfection through chlorine addition</td>
                <td>Break-point chlorination, continuous dosing, shock chlorination</td>
            </tr>
            <tr>
                <td>Filtration</td>
                <td>Physical removal of suspended particles</td>
                <td>Sand filtration, membrane filtration, cartridge filters</td>
            </tr>
            <tr>
                <td>UV/Chemical Disinfection</td>
                <td>Non-chlorine disinfection methods</td>
                <td>UV sterilization, ozone treatment, electrochlorination</td>
            </tr>
            <tr>
                <td>Other</td>
                <td>Custom or specialized interventions</td>
                <td>Site-specific solutions, experimental treatments</td>
            </tr>
        </tbody>
    </table>

    <h3 class="subsection-title" id="section-10-3">10.3 Effectiveness Measurement</h3>
    <p>
        Each completed intervention is evaluated using before-and-after water quality measurements.
        The system calculates improvement percentage based on the targeted parameter:
    </p>

    <div class="pseudocode"><span class="keyword">FUNCTION</span> <span class="function">calculate_intervention_effectiveness</span>(intervention):
    before_value = intervention.before_value
    after_value = intervention.after_value
    parameter = intervention.parameter_targeted  <span class="comment">// e.g., "turbidity", "ph", "chlorine"</span>

    <span class="comment">// For parameters where lower is better (turbidity, TDS, coliform)</span>
    <span class="keyword">IF</span> parameter <span class="keyword">IN</span> [turbidity, tds, coliform, iron, nitrate, arsenic] <span class="keyword">THEN</span>
        improvement_percent = ((before_value - after_value) / before_value) * 100

    <span class="comment">// For parameters where target range is optimal (pH, chlorine)</span>
    <span class="keyword">ELSE IF</span> parameter <span class="keyword">IN</span> [ph, chlorine] <span class="keyword">THEN</span>
        before_deviation = ABS(before_value - target_value)
        after_deviation = ABS(after_value - target_value)
        improvement_percent = ((before_deviation - after_deviation) / before_deviation) * 100

    <span class="keyword">RETURN</span> improvement_percent</div>

    <h3 class="subsection-title" id="section-10-4">10.4 Deployment Results</h3>

    <div class="alert alert-info mb-3">
        <strong><i class="bi bi-info-circle"></i> Note on Intervention Data:</strong>
        The intervention results shown below are based on <strong>simulated scenarios</strong> and do not represent actual field interventions.
        These simulated results are included for reference purposes to demonstrate the intervention tracking framework's capabilities
        and the types of metrics that would be collected in operational deployment.
    </div>

    <p>
        The intervention tracking system has recorded <strong>{{ stats.total_interventions }}</strong> simulated interventions
        across all monitored sites, with <strong>{{ stats.completed_interventions }}</strong> completed simulated interventions
        providing measurable outcome data for framework validation.
    </p>

    <table class="data-table">
        <thead>
            <tr>
                <th>Metric</th>
                <th>Value</th>
                <th>Description</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>Total Simulated Interventions</td>
                <td>{{ stats.total_interventions }}</td>
                <td>All simulated interventions across all sites (planned, in-progress, completed) - for reference</td>
            </tr>
            <tr>
                <td>Completed Simulated Interventions</td>
                <td>{{ stats.completed_interventions }}</td>
                <td>Simulated interventions with verified completion and outcome data - for reference</td>
            </tr>
            <tr>
                <td>Average Effectiveness (Simulated)</td>
                <td>{{ stats.avg_intervention_effectiveness }}%</td>
                <td>Mean improvement in targeted water quality parameter (simulated scenarios)</td>
            </tr>
            <tr>
                <td>Total Simulated Investment</td>
                <td>Rs. {{ '{:,.0f}'.format(stats.total_intervention_cost) }}</td>
                <td>Cumulative cost of all simulated interventions with recorded expenses - for reference</td>
            </tr>
        </tbody>
    </table>

    <h3 class="subsection-title" id="section-10-5">10.5 Integration with ML Pipeline</h3>
    <p>
        The intervention module integrates with the ML pipeline through automated treatment recommendations:
    </p>

    <div class="pseudocode"><span class="keyword">FUNCTION</span> <span class="function">recommend_intervention</span>(contamination_prediction, site, budget):
    contamination_type = contamination_prediction.predicted_type

    <span class="comment">// Match contamination type to treatment methods</span>
    <span class="keyword">IF</span> contamination_type == "sewage_ingress" <span class="keyword">THEN</span>
        recommended_methods = [chlorination, uv_disinfection, infrastructure_repair]
    <span class="keyword">ELSE IF</span> contamination_type == "runoff_sediment" <span class="keyword">THEN</span>
        recommended_methods = [filtration, cleaning, sedimentation]
    <span class="keyword">ELSE IF</span> contamination_type == "salt_intrusion" <span class="keyword">THEN</span>
        recommended_methods = [reverse_osmosis, source_switching]
    <span class="keyword">ELSE IF</span> contamination_type == "pipe_corrosion" <span class="keyword">THEN</span>
        recommended_methods = [pipe_replacement, corrosion_inhibitor, pH_adjustment]
    <span class="keyword">ELSE IF</span> contamination_type == "disinfectant_decay" <span class="keyword">THEN</span>
        recommended_methods = [re_chlorination, booster_stations, residence_time_reduction]

    <span class="comment">// Filter by budget and historical effectiveness</span>
    affordable_methods = FILTER(recommended_methods, cost <= budget)
    ranked_methods = SORT_BY_EFFECTIVENESS(affordable_methods, site.intervention_history)

    <span class="keyword">RETURN</span> ranked_methods[0:3]  <span class="comment">// Top 3 recommendations</span></div>

    <p>
        This integration ensures that ML-based contamination detection directly informs actionable remediation strategies,
        creating a data-driven feedback loop that continuously improves water quality management.
    </p>

    <div class="page-break"></div>

    <!-- Results -->
    <h2 class="section-title" id="section-11">11. Results Summary</h2>

    <h3 class="subsection-title" id="section-11-1">11.1 Model Performance Metrics</h3>
    <table class="data-table">
        <thead>
            <tr>
                <th>Model</th>
                <th>Metric</th>
                <th>Value</th>
                <th>Reference Target</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>Site Risk Classifier</td>
                <td>Predictions</td>
                <td>{{ stats.risk_predictions }}</td>
                <td>-</td>
            </tr>
            <tr>
                <td>Contamination Classifier</td>
                <td>Avg Confidence</td>
                <td>{{ stats.avg_confidence }}%</td>
                <td>82% F1</td>
            </tr>
            <tr>
                <td>Water Quality Forecaster</td>
                <td>R² Score</td>
                <td>{{ stats.avg_r2 }}</td>
                <td>0.76-0.81</td>
            </tr>
            <tr>
                <td>Cost Optimizer</td>
                <td>Cost Reduction</td>
                <td>{{ stats.avg_cost_reduction }}%</td>
                <td>62%</td>
            </tr>
            <tr>
                <td>Cost Optimizer</td>
                <td>Detection Rate</td>
                <td>{{ stats.avg_detection_rate }}%</td>
                <td>&gt;90%</td>
            </tr>
            <tr>
                <td>WQI Calculator</td>
                <td>Average WQI</td>
                <td>{{ stats.avg_wqi }}</td>
                <td>-</td>
            </tr>
        </tbody>
    </table>

    <h3 class="subsection-title" id="section-11-2">11.2 Geographic Coverage</h3>
    <table class="data-table">
        <thead>
            <tr>
                <th>State</th>
                <th>Sites</th>
                <th>Samples</th>
            </tr>
        </thead>
        <tbody>
            {% for state in stats.state_breakdown %}
            <tr>
                <td>{{ state.name }}</td>
                <td>{{ state.sites }}</td>
                <td>{{ state.samples }}</td>
            </tr>
            {% endfor %}
        </tbody>
    </table>

    <!-- Conclusion -->
    <h2 class="section-title" id="section-12">12. Conclusion and Future Work</h2>

    <h3 class="subsection-title" id="section-12-1">12.1 Implementation Roadmap</h3>

    <table class="table table-bordered">
        <thead>
            <tr>
                <th style="width: 12%;">Phase</th>
                <th style="width: 15%;">Status</th>
                <th style="width: 35%;">Key Objectives</th>
                <th style="width: 23%;">Key Achievements / Milestones</th>
                <th style="width: 15%;">Timeline</th>
            </tr>
        </thead>
        <tbody>
            <!-- Phase 1 -->
            <tr>
                <td><strong>Phase 1</strong><br>Preliminary Trials</td>
                <td><span class="badge bg-success">COMPLETED</span></td>
                <td>
                    • LAS robotic platform proof-of-concept<br>
                    • IoT sensor preliminary trials<br>
                    • Bulk data import infrastructure setup
                </td>
                <td>
                    • {{ stats.total_sites }} public sites onboarded<br>
                    • Baseline dataset: {{ stats.total_samples }} samples<br>
                    • Data standardization pipeline validated
                </td>
                <td>Completed<br>{{ stats.phase1_timeframe }}</td>
            </tr>

            <!-- Phase 2 -->
            <tr>
                <td><strong>Phase 2</strong><br>ML Development</td>
                <td><span class="badge bg-success">COMPLETED</span></td>
                <td>
                    • Train and validate 5 ML models<br>
                    • Performance optimization and validation<br>
                    • Production deployment readiness
                </td>
                <td>
                    • <strong>84.6%</strong> risk prediction accuracy<br>
                    • <strong>73.4%</strong> contamination classification<br>
                    • <strong>R²=0.94</strong> forecasting performance<br>
                    • <strong>{{ stats.avg_cost_reduction }}%</strong> cost reduction
                </td>
                <td>Completed<br>{{ stats.phase2_timeframe }}</td>
            </tr>

            <!-- Phase 3 -->
            <tr>
                <td><strong>Phase 3</strong><br>Ramp-Up Deployment</td>
                <td><span class="badge bg-primary">IN PROGRESS</span></td>
                <td>
                    • Expand LAS to 10-15 additional sites<br>
                    • Deploy IoT to 25-50 residential systems<br>
                    • Real-time ML model integration<br>
                    • Sensor accuracy validation vs. laboratory
                </td>
                <td>
                    • Deployment expansion underway<br>
                    • Real-time data pipeline integration<br>
                    • Cross-validation framework operational
                </td>
                <td>Current<br>{{ stats.phase3_timeframe }}</td>
            </tr>

            <!-- Phase 4 -->
            <tr>
                <td><strong>Phase 4</strong><br>Scale-Up LAS</td>
                <td><span class="badge bg-warning">PLANNED</span></td>
                <td>
                    • Deploy to 50+ public water bodies<br>
                    • Multi-state geographic expansion<br>
                    • Navigation algorithm optimization<br>
                    • Automated maintenance protocols
                </td>
                <td>
                    <em>Target Specifications:</em><br>
                    • Fiberglass hull with GPS navigation<br>
                    • 6-sensor suite + computer vision<br>
                    • Cost: ₹2.7L per unit<br>
                    • 14-day autonomous operation
                </td>
                <td>Next<br>12-18 months</td>
            </tr>

            <!-- Phase 5 -->
            <tr>
                <td><strong>Phase 5</strong><br>Scale-Up IoT</td>
                <td><span class="badge bg-warning">PLANNED</span></td>
                <td>
                    • Deploy to 500+ residential water systems<br>
                    • Edge computing optimization<br>
                    • User-facing mobile app development<br>
                    • Automated sensor health monitoring
                </td>
                <td>
                    <em>Target Specifications:</em><br>
                    • Edge computing: Raspberry Pi<br>
                    • Connectivity: LoRaWAN/NB-IoT<br>
                    • Cost: ₹8,000-15,000 per household<br>
                    • Sampling: 15-minute intervals
                </td>
                <td>18-24<br>months</td>
            </tr>

            <!-- Phase 6 -->
            <tr>
                <td><strong>Phase 6</strong><br>Hybrid Operations</td>
                <td><span class="badge bg-info">ONGOING</span></td>
                <td>
                    • Permanent coexistence of manual and autonomous data<br>
                    • Multi-source data fusion (laboratory > LAS > IoT)<br>
                    • Automated anomaly → manual verification loops<br>
                    • Expert review of critical ML predictions
                </td>
                <td>
                    • Hybrid model operational<br>
                    • Data reliability weighting implemented<br>
                    • Human oversight protocols established<br>
                    • Government integration active
                </td>
                <td>Long-term<br>operational<br>mode</td>
            </tr>
        </tbody>
    </table>

    <p class="mt-3">
        <strong>Key Insight:</strong> The transition to autonomous operation is <strong>incremental, not revolutionary</strong>.
        Manual data collection established the intelligent foundation through bulk imports and laboratory validation (Phases 1-2).
        Autonomous systems enhance monitoring coverage and frequency (Phases 3-5), while human expertise and laboratory validation
        remain essential for water safety decisions (Phase 6 ongoing). This hybrid approach balances technological innovation
        with public health responsibility.
    </p>

    <h3 class="subsection-title">12.2 Current Achievements (Phases 1-2 Complete)</h3>

    <p>
        The Jal Sarovar framework has successfully completed foundational development and validation phases:
    </p>

    <ul>
        <li>
            <strong>Data Infrastructure:</strong> Established comprehensive dataset of {{ stats.total_samples }} water quality
            samples from {{ stats.total_sites }} public monitoring sites, spanning {{ stats.date_range_years }} years of
            historical data through bulk imports (CSV, API, manual entry)
        </li>
        <li>
            <strong>ML Model Validation:</strong> Five production-ready models achieve {{ stats.avg_risk_accuracy }}% risk
            prediction accuracy, {{ stats.avg_contamination_accuracy }}% contamination classification, and R²={{ stats.avg_r2 }}
            forecasting performance on historical data
        </li>
        <li>
            <strong>Proof-of-Concept Deployment:</strong> LAS robotic platform and IoT sensor preliminary trials successfully
            validated autonomous data collection capabilities and sensor accuracy against laboratory standards
        </li>
        <li>
            <strong>Cost Optimization:</strong> Demonstrated {{ stats.avg_cost_reduction }}% potential reduction in testing costs
            through intelligent resource allocation while maintaining {{ stats.avg_detection_rate }}% contamination detection rate
        </li>
        <li>
            <strong>Government Integration:</strong> Framework aligned with Mission Amrit Sarovar (68,000+ national water bodies),
            Jal Jeevan Mission (rural drinking water), and state/municipal monitoring programs
        </li>
    </ul>

    <h3 class="subsection-title">12.3 Future Work and Scaling Strategy (Phases 3-6)</h3>

    <p>
        The roadmap focuses on <strong>incremental scaling</strong> of autonomous monitoring infrastructure while
        maintaining the hybrid data collection model:
    </p>

    <h4>Near-Term: Phase 3 Ramp-Up (Current Focus)</h4>
    <ul>
        <li>Expand LAS deployment to 10-15 additional public water body sites with real-time ML integration for
            site risk assessment and contamination detection</li>
        <li>Deploy IoT sensors to 25-50 additional residential water systems with real-time anomaly detection and
            alert notifications</li>
        <li>Continue bulk import focus as primary data source while validating autonomous sensor accuracy through
            cross-validation against laboratory test results</li>
        <li>Integrate LAS and IoT sensor data streams with Phase 2 production ML models for real-time predictions</li>
    </ul>

    <h4>Mid-Term: Phases 4-5 Scale-Up (12-24 Months)</h4>
    <ul>
        <li><strong>LAS Expansion:</strong> Deploy to 50+ public water bodies across Gujarat, Maharashtra, and additional
            states with optimized navigation algorithms for diverse water body types (stepwells, tanks, ponds, lakes, rivers)</li>
        <li><strong>IoT Network Growth:</strong> Expand inline sensor network to 500+ residential water systems with edge
            computing optimization, user-facing mobile app for real-time alerts, and automated sensor health monitoring</li>
        <li><strong>Integration Strategy:</strong> Autonomous data streams supplement, not replace, manual laboratory
            testing—preserving accuracy standards while increasing monitoring frequency and geographic coverage</li>
    </ul>

    <h4>Long-Term: Phase 6 Hybrid Operational Model (Permanent)</h4>
    <ul>
        <li><strong>Data Source Coexistence:</strong> Manual import channels (CSV uploads, API integration, laboratory submissions)
            continue alongside autonomous streams (LAS robots, IoT sensors, third-party smart meters)</li>
        <li><strong>Data Fusion Framework:</strong> ML models weight multi-source data by reliability hierarchy:
            Laboratory > LAS > IoT > Field Kits, ensuring critical decisions prioritize highest-quality data</li>
        <li><strong>Human-AI Collaboration:</strong> Automated anomaly detection triggers manual verification sampling;
            expert review required for critical predictions; laboratory validation remains gold standard</li>
        <li><strong>Government Scalability:</strong> Framework designed to support Mission Amrit Sarovar's 68,000+ water body
            target and Jal Jeevan Mission's rural household coverage through cost-effective hybrid monitoring</li>
    </ul>

    <div class="alert alert-info mt-3">
        <strong>Strategic Vision:</strong> The Jal Sarovar framework demonstrates that effective water quality monitoring
        at scale requires <strong>hybrid intelligence</strong>—combining the coverage and frequency advantages of autonomous
        systems with the accuracy and expertise of traditional laboratory methods and human oversight. This approach ensures
        technological innovation serves public health goals responsibly and sustainably.
    </div>

    <!-- References Section -->
    <div class="page-break"></div>
    <h2 class="section-title" id="section-13">13. References</h2>

    <div class="references-section">
        <ol class="references-list">
            <li>
                World Health Organization, "Progress on household drinking water, sanitation and hygiene 2000-2017:
                Special focus on inequalities," WHO/UNICEF Joint Monitoring Programme, 2019.
            </li>
            <li>
                Central Pollution Control Board (CPCB), "Status of Water Quality in India - 2020,"
                Ministry of Environment, Forest and Climate Change, Government of India, 2020.
            </li>
            <li>
                World Health Organization, "Guidelines for drinking-water quality: fourth edition incorporating
                the first addendum," Geneva: World Health Organization, 2017.
            </li>
            <li>
                Bureau of Indian Standards, "IS 10500:2012 Drinking Water - Specification (Second Revision),"
                BIS, New Delhi, 2012.
            </li>
            <li>
                T. P. Lambrou, C. C. Anastasiou, C. G. Panayiotou, and M. M. Polycarpou, "A low-cost sensor network
                for real-time monitoring and contamination detection in drinking water distribution systems,"
                <em>IEEE Sensors Journal</em>, vol. 14, no. 8, pp. 2765-2772, 2014.
            </li>
            <li>
                S. Geetha and S. Gouthami, "Internet of things enabled real time water quality monitoring system,"
                <em>Smart Water</em>, vol. 2, no. 1, pp. 1-19, 2016.
            </li>
            <li>
                J. Manley and S. Willcox, "The Wave Glider: A persistent platform for ocean science,"
                in <em>Proc. IEEE OCEANS Conf.</em>, 2010, pp. 1-5.
            </li>
            <li>
                M. Dunbabin and L. Marques, "Robots for environmental monitoring: Significant advancements and applications,"
                <em>IEEE Robotics & Automation Magazine</em>, vol. 19, no. 1, pp. 24-39, 2012.
            </li>
            <li>
                K. Chen et al., "Comparative analysis of surface water quality prediction performance and identification
                of key water parameters using different machine learning models based on big data,"
                <em>Water Research</em>, vol. 171, p. 115454, 2020.
            </li>
            <li>
                A. Kulkarni, D. Mukherjee, and K. N. Sharma, "Water quality classification using machine learning,"
                <em>Int. J. Eng. Res. Technol.</em>, vol. 10, no. 3, pp. 581-585, 2021.
            </li>
            <li>
                Y. Zhang, P. Gao, Y. Li, and X. Fang, "A machine learning based approach to identify protected area
                management effectiveness," <em>Sustainability</em>, vol. 11, no. 13, p. 3690, 2019.
            </li>
            <li>
                U. Ahmed et al., "Prediction of drinking water quality using machine learning,"
                in <em>Proc. IEEE Int. Conf. Commun. Signal Process. Appl.</em>, 2019, pp. 1-5.
            </li>
            <li>
                H. Wang, H. Hu, H. Li, and B. Li, "Water quality prediction based on long short-term memory neural network,"
                in <em>Proc. Int. Conf. Water Resources Environ.</em>, 2021, pp. 234-241.
            </li>
            <li>
                S. Seo et al., "Automated plankton classification with a mask-convolutional neural network,"
                <em>Applied Sciences</em>, vol. 11, no. 9, p. 4332, 2021.
            </li>
            <li>
                M. H. Gholizadeh, A. M. Melesse, and L. Reddi, "A comprehensive review on water quality parameters
                estimation using remote sensing techniques," <em>Sensors</em>, vol. 16, no. 8, p. 1298, 2016.
            </li>
            <li>
                L. Breiman, "Random forests," <em>Machine Learning</em>, vol. 45, no. 1, pp. 5-32, 2001.
            </li>
            <li>
                T. Chen and C. Guestrin, "XGBoost: A scalable tree boosting system,"
                in <em>Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining</em>, 2016, pp. 785-794.
            </li>
            <li>
                S. M. Lundberg and S.-I. Lee, "A unified approach to interpreting model predictions,"
                in <em>Advances in Neural Information Processing Systems</em>, 2017, pp. 4765-4774.
            </li>
            <li>
                C. E. Rasmussen and C. K. I. Williams, <em>Gaussian Processes for Machine Learning</em>.
                Cambridge, MA: MIT Press, 2006.
            </li>
            <li>
                J. Snoek, H. Larochelle, and R. P. Adams, "Practical Bayesian optimization of machine learning algorithms,"
                in <em>Advances in Neural Information Processing Systems</em>, 2012, pp. 2951-2959.
            </li>
            <li>
                F. T. Liu, K. M. Ting, and Z.-H. Zhou, "Isolation forest,"
                in <em>Proc. IEEE Int. Conf. Data Mining</em>, 2008, pp. 413-422.
            </li>
            <li>
                E. S. Page, "Continuous inspection schemes," <em>Biometrika</em>, vol. 41, no. 1/2, pp. 100-115, 1954.
            </li>
            <li>
                M. Sandler et al., "MobileNetV2: Inverted residuals and linear bottlenecks,"
                in <em>Proc. IEEE Conf. Comput. Vis. Pattern Recognit.</em>, 2018, pp. 4510-4520.
            </li>
        </ol>
    </div>

    <style>
        .references-section {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 5px;
            margin-top: 20px;
        }
        .references-list {
            font-size: 0.9em;
            line-height: 1.6;
            padding-left: 20px;
        }
        .references-list li {
            margin-bottom: 12px;
        }
        .references-list em {
            font-style: italic;
        }
    </style>
</div>
{% endblock %}
